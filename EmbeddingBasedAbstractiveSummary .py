import asyncio
import hashlib
import json
import logging
import re
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import fitz
import os, sys, locale
if os.name == "nt":                       # Windows í•œì •
    locale.setlocale(locale.LC_ALL, "")   # í˜„ì¬ ë¡œìº˜ ìœ ì§€
    sys.stdout.reconfigure(encoding="utf-8", errors="replace")
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

# aiohttp ì‚¬ìš©ìœ¼ë¡œ ë³€ê²½í•˜ì—¬ í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°
import aiohttp
from aiohttp import ClientError, ClientResponseError, ClientTimeout

# ë¡œê±° ì„¤ì •
logger = logging.getLogger(__name__)

@dataclass
class TextChunk:
    text: str
    token_count: int
    priority: float = 1.0

    def __post_init__(self):
        if not self.text.strip():
            raise ValueError("ë¹ˆ í…ìŠ¤íŠ¸ ì²­í¬ëŠ” í—ˆìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤")

class ResponseCache:
    def __init__(self, max_size: int = 1000, ttl: int = 3600):
        from collections import OrderedDict
        import time

        self.cache = OrderedDict()  # LRU ìºì‹œë¡œ ì‚¬ìš©
        self.timestamps = {}        # íƒ€ì„ìŠ¤íƒ¬í”„ ì €ì¥
        self.max_size = max_size
        self.ttl = ttl
        self.hits = 0
        self.misses = 0

    async def get(self, key: str) -> Optional[str]:
        import time

        if key in self.cache:
            # monotonic ì‹œê³„ ì‚¬ìš© (ì‹œìŠ¤í…œ ì‹œê°„ ë³€ê²½ì— ì˜í–¥ ë°›ì§€ ì•ŠìŒ)
            if time.monotonic() - self.timestamps[key] < self.ttl:
                # LRU ì—…ë°ì´íŠ¸ - í•­ëª© ì¬ë°°ì¹˜
                value = self.cache.pop(key)
                self.cache[key] = value  # ë§¨ ë’¤ë¡œ ì´ë™
                self.hits += 1
                return value
            else:
                # TTL ë§Œë£Œëœ í•­ëª© ì œê±°
                self.cache.pop(key, None)
                self.timestamps.pop(key, None)

        self.misses += 1
        return None

    async def put(self, key: str, value: str):
        import time

        # LRU ìºì‹œ í¬ê¸° ì œí•œ (ì‚½ì… ì „ í™•ì¸)
        if len(self.cache) >= self.max_size and key not in self.cache:
            # ê°€ì¥ ì˜¤ë˜ ì‚¬ìš©ë˜ì§€ ì•Šì€ í•­ëª© ì œê±° (OrderedDictì˜ ì²« í•­ëª©)
            oldest_key, _ = self.cache.popitem(last=False)
            self.timestamps.pop(oldest_key, None)

        # í•­ëª© ì¶”ê°€/ê°±ì‹ 
        self.cache[key] = value
        self.timestamps[key] = time.monotonic()  # monotonic ì‹œê³„ ì‚¬ìš©

    async def close(self):
        self.cache.clear()
        self.timestamps.clear()

    def get_stats(self) -> dict:
        total = self.hits + self.misses
        hit_rate = self.hits / total if total > 0 else 0
        return {
            "hits": self.hits,
            "misses": self.misses,
            "hit_rate": f"{hit_rate:.3f}",
            "cache_size": len(self.cache)
        }


class AsyncPDFProcessor:
    def __init__(self, concurrency: int = 8):
        self.sem = asyncio.Semaphore(concurrency)

    async def extract(self, pdf_path: str) -> str:
        """
        PDF ì „ì²´ë¥¼ í•œ ë²ˆë§Œ ì—° ë’¤, í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ìˆœì°¨-ìŠ¤ë ˆë“œë¡œ ì¶”ì¶œ.
        ê° worker ëŠ” *ìê¸°ë§Œì˜* Document ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ thread-safe.
        """
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"PDF íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {pdf_path}")

        loop = asyncio.get_running_loop()

        # â”€â”€ í˜ì´ì§€ë³„ ì‘ì—… í•¨ìˆ˜ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        def _page_text(idx: int) -> str:
            # ê° ìŠ¤ë ˆë“œê°€ ë…ë¦½ì ìœ¼ë¡œ ë¬¸ì„œë¥¼ ì—´ê³  ë‹«ëŠ”ë‹¤ â†’ ì•ˆì „
            with fitz.open(pdf_path) as doc:
                return doc.load_page(idx).get_text("text").strip()

        # í•œë²ˆ ì—´ì–´ì„œ í˜ì´ì§€ ìˆ˜ë§Œ ì•Œì•„ë‚¸ë‹¤
        with fitz.open(pdf_path) as doc:
            page_count = len(doc)
        if page_count == 0:
            raise ValueError("PDF íŒŒì¼ì— í˜ì´ì§€ê°€ ì—†ìŠµë‹ˆë‹¤")

        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ì‹¤í–‰ ì œí•œ
        async def _run(idx: int) -> str:
            async with self.sem:
                return await loop.run_in_executor(None, _page_text, idx)

        # ë³‘ë ¬ ì‹¤í–‰
        pages = await asyncio.gather(*[_run(i) for i in range(page_count)],
                                     return_exceptions=True)

        # ìœ íš¨ í˜ì´ì§€ ìˆ˜ì§‘
        valid_pages = []
        for i, page in enumerate(pages):
            if isinstance(page, Exception):
                logger.error(f"[PDF] í˜ì´ì§€ {i} ì²˜ë¦¬ ì‹¤íŒ¨: {page}")
            elif page:
                valid_pages.append(page)

        if not valid_pages:
            raise ValueError("PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")

        return "\n\n".join(valid_pages)

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬ - ì´ì œ ê° ì‘ì—…ìê°€ ë…ë¦½ì ìœ¼ë¡œ ë¬¸ì„œë¥¼ ë‹«ìœ¼ë¯€ë¡œ í•„ìš” ì—†ìŒ"""

class AdaptiveTokenManager:
    def __init__(self, model_path: str = "./", ctx_len: int = 2048):
        self.ctx_len = ctx_len
        self.model_path = model_path

    def count_tokens(self, text: str) -> int:
        """ê°„ë‹¨í•œ í† í° ì¹´ìš´íŒ… (ë¹ˆ ë¬¸ìì—´ ê²€ì¦ ì¶”ê°€)"""
        if not text or not text.strip():
            # ë¹ˆ í…ìŠ¤íŠ¸ëŠ” 0í† í°ìœ¼ë¡œ ì²˜ë¦¬ (ì—ëŸ¬ ë°©ì§€ìš©)
            return 0
        # ìµœì†Œ 1í† í° ì´ìƒ ë³´ì¥
        return max(1, int(len(text.split()) * 1.3))


    def create_adaptive_chunks(
        self, 
        text: str, 
        target_summary_length: int = 200
    ) -> Tuple[List[TextChunk], Dict[str, Any]]:
        """ì ì‘ì  ì²­í‚¹ ì•Œê³ ë¦¬ì¦˜"""
        
        total_tokens = self.count_tokens(text)
        if total_tokens <= self.ctx_len * 0.7:
            return [TextChunk(text, int(total_tokens))], {"strategy": "single_chunk"}
        
        # ë¬¸ë‹¨ ê¸°ë°˜ ë¶„í• 
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        if not paragraphs:
            paragraphs = [text]
        
        chunks = []
        current_chunk = ""
        current_tokens = 0
        max_chunk_tokens = int(self.ctx_len * 0.6)
        
        for para in paragraphs:
            para_tokens = self.count_tokens(para)
            
            if current_tokens + para_tokens <= max_chunk_tokens:
                current_chunk += para + "\n\n"
                current_tokens += para_tokens
            else:
                if current_chunk:
                    chunks.append(TextChunk(current_chunk.strip(), int(current_tokens)))
                
                if para_tokens > max_chunk_tokens:
                    # ê¸´ ë¬¸ë‹¨ì€ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
                    sentences = re.split(r'[.!?]+', para)
                    temp_chunk = ""
                    temp_tokens = 0
                    
                    for sent in sentences:
                        if not sent.strip():
                            continue
                        sent_tokens = self.count_tokens(sent)
                        
                        if temp_tokens + sent_tokens <= max_chunk_tokens:
                            temp_chunk += sent + ". "
                            temp_tokens += sent_tokens
                        else:
                            if temp_chunk:
                                chunks.append(TextChunk(temp_chunk.strip(), int(temp_tokens)))
                            temp_chunk = sent + ". "
                            temp_tokens = sent_tokens
                    
                    if temp_chunk:
                        current_chunk = temp_chunk
                        current_tokens = temp_tokens
                    else:
                        current_chunk = ""
                        current_tokens = 0
                else:
                    current_chunk = para + "\n\n"
                    current_tokens = para_tokens
        
        if current_chunk:
            chunks.append(TextChunk(current_chunk.strip(), int(current_tokens)))
        
        # ìš°ì„ ìˆœìœ„ ê³„ì‚°
        for i, chunk in enumerate(chunks):
            chunk.priority = 1.0 + (0.1 * (len(chunks) - i))
        
        allocation = {
            "strategy": "adaptive",
            "total_chunks": len(chunks),
            "avg_chunk_size": sum(c.token_count for c in chunks) / len(chunks),
            "max_chunk_size": max(c.token_count for c in chunks),
            "min_chunk_size": min(c.token_count for c in chunks)
        }
        
        return chunks, allocation

class OptimizedTritonClient:
    MAX_SERVER_TOKENS = 2047

    def __init__(self, url: str, batch: int = 16, timeout: float = 60.0, max_connections: int = 32, tokenizer=None):
        self.url = url.rstrip("/")
        self.sem = asyncio.Semaphore(batch)
        self.cache = ResponseCache()
        self.timeout = timeout
        self.batch = batch
        self.max_connections = max_connections
        self._session = None
        self.tokenizer = tokenizer

    def _trim_for_server(self, user_text: str) -> str:
        """
        âœ”  chat_template ê°€ ë¶™ì€ **ìµœì¢…** í† í° ìˆ˜ê°€ 2 047 ì´í•˜ê°€ ë˜ë„ë¡
           user_text(=ì„œë²„ì— ë³´ë‚¼ query) ë¥¼ ê°€ìš´ë°ë¥¼ ì˜ë¼ ì¶•ì•½.
        """
        tok = self.tokenizer
        if not tok:
            return user_text

        # â‘  í…œí”Œë¦¿ â€˜ë¨¸ë¦¬Â·ê¼¬ë¦¬â€™ ê¸¸ì´(ê³ ì •ë¶„) ë¯¸ë¦¬ ê³„ì‚°
        empty_tpl = tok.apply_chat_template(
            [{"role": "user", "content": ""}],
            tokenize=False, add_generation_prompt=True
        )
        head_tail_tokens = len(tok.encode(empty_tpl, add_special_tokens=False))

        # â‘¡ user_text ë¥¼ í† í°í™”
        ids_user = tok.encode(user_text, add_special_tokens=False)
        max_user_tokens = self.MAX_SERVER_TOKENS - head_tail_tokens - 2   # 2-í† í° ì—¬ìœ 

        if len(ids_user) <= max_user_tokens:
            return user_text                        # ì´ë¯¸ ì•ˆì „

        # â‘¢ ê°€ìš´ë° ì˜ë¼ë‚´ê¸°
        keep_front = max_user_tokens // 2
        keep_back  = max_user_tokens - keep_front
        new_ids = ids_user[:keep_front] + ids_user[-keep_back:]

        return tok.decode(new_ids, skip_special_tokens=False)

    @property
    def session(self) -> aiohttp.ClientSession:
        if self._session is None or self._session.closed:
            connector = aiohttp.TCPConnector(
                limit=self.max_connections,
                limit_per_host=self.max_connections,
                keepalive_timeout=30.0,
                enable_cleanup_closed=True
            )
            timeout = aiohttp.ClientTimeout(total=self.timeout, connect=10.0)
            self._session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                raise_for_status=True  # ìë™ìœ¼ë¡œ HTTP ì˜¤ë¥˜ ë°œìƒ
            )
        return self._session

    async def __aenter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì… - ì„¸ì…˜ ì´ˆê¸°í™” ë³´ì¥"""
        # ì„¸ì…˜ì´ ë‹«í˜”ê±°ë‚˜ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± (ì†ì„± ì ‘ê·¼ì í™œìš©)
        if getattr(self, '_session', None) is None or self._session.is_closed:
            _ = self.session  # ì„¸ì…˜ ì´ˆê¸°í™” íŠ¸ë¦¬ê±°
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ - ì•ˆì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.close()

    async def close(self):
        """ì•ˆì „í•œ ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            if self._session and not self._session.closed:
                await self._session.close()
                # ì„¸ì…˜ì´ ì™„ì „íˆ ì •ë¦¬ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
                await asyncio.sleep(0.1)
        except Exception as e:
            logger.debug(f"ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
        finally:
            self._session = None
        
        try:
            await self.cache.close()
        except Exception as e:
            logger.debug(f"ìºì‹œ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

    async def generate_parallel_optimized(
        self,
        prompts: list[str],
        *,
        max_new_tokens: int = 160,
        dynamic_batch_size: bool = True
    ) -> list[str]:
        """
        ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •ê³¼ íŒŒì´í”„ë¼ì´ë‹ì„ í†µí•œ ìµœì í™”ëœ ë³‘ë ¬ ì²˜ë¦¬
        """
        if not prompts:
            return []

        # ë™ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        if dynamic_batch_size:
            avg_prompt_length = sum(len(p) for p in prompts) / len(prompts)
            if avg_prompt_length < 500:
                effective_batch = min(self.batch * 2, 32)  # ì§§ì€ í”„ë¡¬í”„íŠ¸ëŠ” ë” ë§ì´
            else:
                effective_batch = max(self.batch // 2, 4)   # ê¸´ í”„ë¡¬í”„íŠ¸ëŠ” ì ê²Œ
        else:
            effective_batch = self.batch

        # íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì²­í¬ ë¶„í• 
        chunks = [prompts[i:i + effective_batch] for i in range(0, len(prompts), effective_batch)]
        results = [None] * len(prompts)

        async def process_chunk(chunk_prompts: list[str], start_idx: int):
            tasks = []
            for i, prompt in enumerate(chunk_prompts):
                # ê° í”„ë¡¬í”„íŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” íƒœìŠ¤í¬ ìƒì„±
                task = asyncio.create_task(
                    self._generate_single_cached(prompt, max_new_tokens),
                    name=f"prompt_{start_idx + i}"
                )
                tasks.append(task)

            # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
            chunk_results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ë¥¼ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ë°°ì¹˜
            for i, result in enumerate(chunk_results):
                if isinstance(result, Exception):
                    logger.error(f"í”„ë¡¬í”„íŠ¸ {start_idx + i} ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                    results[start_idx + i] = "[Error]"
                else:
                    results[start_idx + i] = result

        # ì²­í¬ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì œí•œ)
        sem = asyncio.Semaphore(3)  # ìµœëŒ€ 3ê°œ ì²­í¬ ë™ì‹œ ì²˜ë¦¬

        async def process_chunk_with_limit(chunk, idx):
            async with sem:
                await process_chunk(chunk, idx * effective_batch)

        # ëª¨ë“  ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ì„¸ë§ˆí¬ì–´ë¡œ ì œí•œ)
        await asyncio.gather(*[
            process_chunk_with_limit(chunk, i) 
            for i, chunk in enumerate(chunks)
        ])

        return [r for r in results if r is not None]

    async def _generate_single_cached(self, prompt: str, max_new_tokens: int) -> str:
        """ìºì‹œë¥¼ í™œìš©í•œ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ - aiohttp ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ ë°©ì‹"""
        # TensorRT-LLM ìµœëŒ€ ì…ë ¥ ì œí•œ ì²˜ë¦¬ (2047 í† í°)
        MAX_INPUT_TOKENS = 2047
        prompt = self._trim_for_server(prompt)
        # í† í¬ë‚˜ì´ì €ê°€ ìˆì„ ê²½ìš°ì—ë§Œ ê¸¸ì´ ì œí•œ ì ìš©
        if self.tokenizer:
            try:
                token_count = len(self.tokenizer(prompt, add_special_tokens=False).input_ids)

                if token_count > MAX_INPUT_TOKENS:
                    logger.warning(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì´ˆê³¼: {token_count} í† í° > {MAX_INPUT_TOKENS} ì œí•œ (ì˜ë¼ëƒ„)")
                    # í† í° ë‹¨ìœ„ë¡œ ìë¥´ê¸° ìœ„í•´ í† í¬ë‚˜ì´ì € ì‚¬ìš©
                    encoded = self.tokenizer(prompt, add_special_tokens=False)
                    truncated_ids = encoded.input_ids[:MAX_INPUT_TOKENS]
                    prompt = self.tokenizer.decode(truncated_ids)
            except Exception as e:
                logger.warning(f"í† í° ê¸¸ì´ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

        key = hashlib.sha256(f"{prompt}:{max_new_tokens}".encode()).hexdigest()

        # ìºì‹œ í™•ì¸
        try:
            cached = await self.cache.get(key)
            if cached is not None:
                return cached
        except Exception as e:
            logger.debug(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
        async with self.sem:
            full_text = []

            # 3ë²ˆê¹Œì§€ ì¬ì‹œë„
            for attempt in range(3):
                try:
                    headers = {"Content-Type": "application/json", "Accept": "text/event-stream"}
                    data = {"text_input": prompt, "max_tokens": max_new_tokens}

                    async with self.session.post(
                        self.url,
                        json=data,
                        headers=headers,
                        timeout=self.timeout
                    ) as response:
                        response.raise_for_status()

                        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ (chunked)
                        async for chunk in response.content.iter_chunked(1024):
                            chunk_text = chunk.decode("utf-8").strip()

                            # ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
                            for line in chunk_text.split('\n'):
                                if not line or line.isspace():
                                    continue

                                # SSE í˜•ì‹ ì²˜ë¦¬
                                if line.startswith("event:"):
                                    continue
                                if line.startswith("data: "):
                                    try:
                                        json_str = line[6:].strip()
                                        data = json.loads(json_str)
                                        text = data.get("text_output", "")
                                        if text:
                                            full_text.append(text)
                                    except json.JSONDecodeError as e:
                                        # ë” ìì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ë¡œê¹…
                                        logger.debug(f"ì‘ë‹µ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {line!r}")
                                        logger.debug(f"JSONDecodeError: {e}, ìœ„ì¹˜: {e.pos}, ë¼ì¸: {e.lineno}, ì—´: {e.colno}")
                                        continue
                                    except KeyError as e:
                                        logger.debug(f"ì‘ë‹µ ë°ì´í„°ì—ì„œ í•„ìš”í•œ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {e}")
                                        continue
                                    except Exception as e:
                                        logger.debug(f"ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                                        continue

                    # ì‘ë‹µ ê²°í•©
                    result = "".join(full_text)
                    if result:
                        await self.cache.put(key, result)
                        return result
                    else:
                        # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
                        raise ValueError("Triton ì„œë²„ê°€ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")

                except Exception as e:
                    error_msg = str(e)
                    if "Prompt length" in error_msg and "exceeds maximum input length" in error_msg:
                        logger.error(f"ì…ë ¥ ê¸¸ì´ ì´ˆê³¼ ì˜¤ë¥˜ (ì„œë²„ ì¸¡): {e}")
                        return "[ì˜¤ë¥˜: ì…ë ¥ ê¸¸ì´ê°€ ì„œë²„ ì œí•œì„ ì´ˆê³¼í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ê°€ ìë™ìœ¼ë¡œ ì˜ë ¸ì§€ë§Œ ì„œë²„ ì¸¡ì—ì„œ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤.]"

                    if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                        logger.error(f"Triton ì„œë²„ í˜¸ì¶œ ì‹¤íŒ¨ (ìµœì¢…): {e}")
                        return f"[ì¶”ë¡  ì˜¤ë¥˜: {str(e)[:50]}]" if len(str(e)) > 50 else f"[ì¶”ë¡  ì˜¤ë¥˜: {e}]"

                    # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                    logger.warning(f"Triton ì„œë²„ í˜¸ì¶œ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt+1}/3): {e}")
                    await asyncio.sleep(0.5 * (attempt + 1))

            # ì¬ì‹œë„ ì‹¤íŒ¨ ì²˜ë¦¬
            return "[Triton ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜]"

class HierarchicalSummarizer:
    def __init__(self, token_mgr: AdaptiveTokenManager, triton_client: OptimizedTritonClient):
        self.token_mgr = token_mgr
        self.triton = triton_client
        
    async def smart_chunking_summary(
        self, 
        text: str, 
        target_length: int = 200
    ) -> Dict[str, Any]:
        """ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ê¸°ë°˜ ê³„ì¸µì  ìš”ì•½"""
        
        chunks, allocation = self.token_mgr.create_adaptive_chunks(text, target_length)
        
        if len(chunks) == 1:
            # ë‹¨ì¼ ì²­í¬ëŠ” ì§ì ‘ ìš”ì•½
            prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_length}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{chunks[0].text}"
            # í† í° ê¸¸ì´ í™•ì¸ (ë””ë²„ê¹…ìš©, í† í¬ë‚˜ì´ì €ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
            if hasattr(self.triton, 'tokenizer') and self.triton.tokenizer:
                try:
                    token_count = len(self.triton.tokenizer(prompt, add_special_tokens=False).input_ids)
                    if token_count > 2000:
                        logger.warning(f"ë‹¨ì¼ ì²­í¬ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {token_count} í† í° (ìë™ ì˜ë¦¼ ì˜ˆì •)")
                except Exception:
                    pass  # í† í¬ë‚˜ì´ì € ì˜¤ë¥˜ëŠ” ë¬´ì‹œ

            summary = await self.triton._generate_single_cached(prompt, target_length)
            
            return {
                "chunk_summaries": [summary],
                "final_summary": summary,
                "processing_method": "direct"
            }
        
        # ë‹¤ì¤‘ ì²­í¬ ì²˜ë¦¬
        chunk_prompts = []
        chars_per_chunk = target_length
        for i, chunk in enumerate(chunks):
            prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ì„ {chars_per_chunk}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš” (íŒŒíŠ¸ {i+1}/{len(chunks)}):\n\n{chunk.text}"
            chunk_prompts.append(prompt)

        # ë³‘ë ¬ ìš”ì•½ ìƒì„± - ë¬¸ì ìˆ˜ë¥¼ í† í° ìˆ˜ë¡œ ë³€í™˜ (í‰ê·  1.5ë°°)
        approx_tokens_per_chunk = max(40, int(chars_per_chunk * 0.67))
        chunk_summaries = await self.triton.generate_parallel_optimized(
            chunk_prompts, 
            max_new_tokens=max(20, approx_tokens_per_chunk)  # ìµœì†Œ 20í† í° ë³´ì¥
        )
        
        # ìµœì¢… í†µí•© ìš”ì•½
        combined_text = "\n\n".join([s for s in chunk_summaries if s and not s.startswith("[ì˜¤ë¥˜")])  # ì˜¤ë¥˜ ì‘ë‹µ í•„í„°ë§

        if not combined_text.strip():
            logger.warning("ëª¨ë“  ì²­í¬ ìš”ì•½ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì§ì ‘ ìš”ì•½ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            # ëª¨ë“  ì²­í¬ ìš”ì•½ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ê°„ë‹¨í•œ ìš”ì•½ ì¶”ì¶œ
            short_text = text[:min(len(text), 2000)]  # ì›ë³¸ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
            final_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_length}ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{short_text}"
        else:
            final_prompt = f"ë‹¤ìŒ ë¶€ë¶„ë³„ ìš”ì•½ë“¤ì„ ì¢…í•©í•˜ì—¬ {target_length}ì ì´ë‚´ì˜ ìµœì¢… ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”:\n\n{combined_text}"

        # ìµœì¢… ìš”ì•½ì— ì¶©ë¶„í•œ í† í° í• ë‹¹ (í•œê¸€ ë¬¸ì:í† í° ë¹„ìœ¨ ê³ ë ¤)
        approx_tokens = int(target_length * 0.8)  # ì—¬ìœ  ìˆê²Œ í† í° í• ë‹¹ (0.67 â†’ 0.8)
        final_summary = await self.triton._generate_single_cached(final_prompt, max(60, approx_tokens))
        
        return {
            "chunk_summaries": chunk_summaries,
            "final_summary": final_summary,
            "processing_method": "hierarchical"
        }

class OptimizedPipeline:
    def __init__(
        self,
        model_path: str = "./",
        triton_url: str = "http://203.250.238.30:8888/v2/models/ensemble/generate_stream",
        ctx_len: int = 2048
    ):
        self.token_mgr = AdaptiveTokenManager(model_path, ctx_len)
        self.pdf_proc = AsyncPDFProcessor()

        # í† í¬ë‚˜ì´ì € ê°€ì ¸ì˜¤ê¸°
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
        if not tokenizer.pad_token:
            tokenizer.pad_token = tokenizer.eos_token
        # í† í¬ë‚˜ì´ì € ì£¼ì…
        self.triton = OptimizedTritonClient(triton_url, batch=16, tokenizer=tokenizer)
        self.summarizer = HierarchicalSummarizer(self.token_mgr, self.triton)


    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì§„ì…ì """
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì¢…ë£Œ - ìì› ì •ë¦¬"""
        await self.close()

    async def process_document_optimized(
        self,
        pdf_path: str,
        target_summary_length: int = 200
    ) -> dict:
        """ìµœì í™”ëœ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""

        try:
            # 1. PDF ì¶”ì¶œ
            try:
                raw_text = await self.pdf_proc.extract(pdf_path)
                if not raw_text.strip():
                    logger.warning(f"PDFì—ì„œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {pdf_path}")
                    print(f"âš ï¸ PDFì—ì„œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {pdf_path}")
                    return {
                        "warning": "PDFì— ì¶”ì¶œ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤",
                        "success": False
                    }
            except Exception as e:
                logger.error(f"PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                print(f"âŒ PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                return {
                    "error": f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}",
                    "success": False
                }

            # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ 
            clean_text = raw_text.strip()
            if not clean_text:
                raise ValueError("ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

            # 3. ì ì‘ì  ì²­í‚¹
            try:
                chunks, allocation = self.token_mgr.create_adaptive_chunks(
                    clean_text, target_summary_length
                )
            except Exception as e:
                logger.error(f"í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹¤íŒ¨: {e}")
                print(f"âŒ ì²­í‚¹ ì˜¤ë¥˜: {e}")
                return {
                    "error": f"í…ìŠ¤íŠ¸ ì²­í‚¹ ì˜¤ë¥˜: {e}",
                    "success": False
                }

            # 4. ìš”ì•½ ì²˜ë¦¬
            try:
                result = await self.summarizer.smart_chunking_summary(
                    clean_text, target_summary_length
                )
            except Exception as e:
                logger.error(f"ìš”ì•½ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                print(f"âŒ ìš”ì•½ ì˜¤ë¥˜: {e}")
                return {
                    "error": f"ìš”ì•½ ì²˜ë¦¬ ì˜¤ë¥˜: {e}",
                    "success": False
                }

            # 5. ì„±ê³µ ê²°ê³¼ ë°˜í™˜
            result.update({
                "token_allocation": allocation,
                "success": True,
                "processing_stats": {
                    "chunks_created": len(chunks),
                    "avg_chunk_size": sum(c.token_count for c in chunks) / len(chunks),
                    "compression_ratio": len(result.get("final_summary", "")) / len(clean_text)
                }
            })

            return result

        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "error": str(e),
                "success": False
            }

    async def close(self):
        """ì•ˆì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            await self.triton.close()
        except Exception as e:
            logger.debug(f"Triton í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
        
        try:
            await self.pdf_proc.close()
        except Exception as e:
            logger.debug(f"PDF í”„ë¡œì„¸ì„œ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

async def main(pdf_path: str):

    # ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì íŒ¨í„´ìœ¼ë¡œ íŒŒì´í”„ë¼ì¸ ì‚¬ìš©
    # (ë¦¬ì†ŒìŠ¤ëŠ” ìë™ìœ¼ë¡œ ì •ë¦¬ë¨)
    async with OptimizedPipeline() as pipeline:
        try:
            # í˜ì´ì§€ ìˆ˜ í™•ì¸
            temp_doc = None
            try:
                temp_doc = fitz.open(pdf_path)
                page_count = len(temp_doc)
                logger.info(f"PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}, ì´ {page_count}í˜ì´ì§€")
                print(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}, ì´ {page_count}í˜ì´ì§€")
            except Exception as e:
                logger.error(f"PDF í˜ì´ì§€ ìˆ˜ í™•ì¸ ì‹¤íŒ¨: {e}")
                print(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path}")
            finally:
                if temp_doc:
                    temp_doc.close()

            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            if not os.path.exists(pdf_path):
                error_msg = f"PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}"
                logger.error(error_msg)
                print(f"âŒ {error_msg}")
                return

            result = await pipeline.process_document_optimized(pdf_path)

            if result.get("success", False):
                logger.info(f"PDF ì²˜ë¦¬ ì„±ê³µ: {pdf_path}")
                print("â–· íŒŒíŠ¸ë³„ ìš”ì•½ --------------------")
                for i, s in enumerate(result.get("chunk_summaries", []), 1):
                    print(f"[{i}] {s}")

                print("\nâœ… ìµœì¢… ìš”ì•½ --------------------")
                print(result.get("final_summary", "ìš”ì•½ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."))

                stats = result.get('processing_stats', {})
                if stats:
                    print(f"\nğŸ“Š ì²˜ë¦¬ í†µê³„: ì²­í¬ {stats.get('chunks_created', 0)}ê°œ, "
                        f"ì••ì¶•ë¥  {stats.get('compression_ratio', 0):.3f}")
                    logger.info(f"ì²˜ë¦¬ í†µê³„: {stats}")
            else:
                error_msg = result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                logger.error(f"PDF ì²˜ë¦¬ ì‹¤íŒ¨: {error_msg}")
                print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {error_msg}")
        except Exception as e:
            logger.exception(f"ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    # í”„ë¡œê·¸ë¨ ì‹œì‘ì ì—ì„œ ë¡œê¹… ì„¤ì • í•œ ë²ˆë§Œ ì´ˆê¸°í™”
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler("pdf_pipeline.log", mode="a")
        ]
    )

    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê±° ì„¤ì •
    transformers_logger = logging.getLogger("transformers")
    transformers_logger.setLevel(logging.ERROR)  # transformers ê²½ê³  ì–µì œ

    sample_pdf = "example3.pdf"

    # Windowsì—ì„œ ì•ˆì „í•œ ì´ë²¤íŠ¸ ë£¨í”„ ì²˜ë¦¬
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(main(sample_pdf))
    except KeyboardInterrupt:
        print("\nâ¬‡ï¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ìš”ì²­ë¨...")
        # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  íƒœìŠ¤í¬ ì •ë¦¬
        pending = asyncio.all_tasks(loop)
        for task in pending:
            task.cancel()
        
        # ì·¨ì†Œëœ íƒœìŠ¤í¬ë“¤ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        if pending:
            loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
    finally:
        try:
            loop.close()
        except Exception as e:
            print(f"ë£¨í”„ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
        print("âœ… ì™„ë£Œ")
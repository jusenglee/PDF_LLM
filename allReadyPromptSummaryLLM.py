"""PDF ë¬¸ì„œ âœ LLM ìš”ì•½ ìµœì í™” íŒŒì´í”„ë¼ì¸
------------------------------------------------
â€¢ ë¹„ë™ê¸° HTTP/2 + ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ Tritonâ€¯Inferenceâ€¯Server í˜¸ì¶œ
â€¢ í˜ì´ì§€â€‘ë³‘ë ¬ PDF ì¶”ì¶œ (fitz)  + ì •í™•í•œ í† í° ê¸°ë°˜ ì²­í¬ ë¶„í• 
â€¢ SQLite ê¸°ë°˜ ì‘ë‹µ ìºì‹œë¡œ ì¬í˜¸ì¶œ ìµœì†Œí™”
â€¢ ì•ˆì „í•œ signalÂ shutdown & graceful connection close
Â 
PythonÂ 3.10Â ì´ìƒ ê¶Œì¥,Â pipÂ installÂ -rÂ requirements.txtÂ (Â fitzÂ /Â PyMuPDF , transformers , httpxÂ [h2]Â â€¦).
"""
from __future__ import annotations

import asyncio
import contextlib
import hashlib
import json
import signal
import sqlite3
from dataclasses import dataclass
from pathlib import Path
from typing import AsyncIterator, Dict, List, Optional

import fitz  # PyMuPDF
import httpx
from transformers import AutoTokenizer
import logging
from contextlib import asynccontextmanager
from datetime import datetime, timedelta
from typing import AsyncIterator, Optional

logger = logging.getLogger(__name__)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. í† í° ê´€ë¦¬ì
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class ChunkInfo:
    text: str
    token_count: int
    sha256: str


class TokenManager:
    """í…ìŠ¤íŠ¸ë¥¼ LLM ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ì— ë§ê²Œ ì²­í¬ë¡œ ë‚˜ëˆ„ëŠ” í—¬í¼."""

    def __init__(self, model_path: str = "./", ctx_len: int = 2048):
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
        self.max_ctx = ctx_len
        # ìƒì„± í† í°Â·í”„ë¡¬í”„íŠ¸ ì—¬ìœ ë¶„ì„ ê°ì•ˆí•´ ëª©í‘œ ì²­í¬ ê¸¸ì´ ì„¤ì •
        self.target_chunk = int(ctx_len * 0.88)  # â‰ˆ1800 for 2â€¯k ctx

    # FastTokenizer â†’ pure C++ ê²½ë¡œ, add_special_tokensÂ False ì¤‘ìš”
    def count(self, text: str) -> int:
        return len(self.tokenizer(text, add_special_tokens=False).input_ids)

    def create_chunks(self, text: str) -> List[ChunkInfo]:
        words = text.split()
        chunks: List[ChunkInfo] = []
        cur_words: List[str] = []
        cur_tok = 0

        for w in words:
            w_tok = self.count(w)
            if cur_tok + w_tok > self.target_chunk:
                chunk_text = " ".join(cur_words)
                chunks.append(self._build_chunk(chunk_text))
                cur_words, cur_tok = [w], w_tok
            else:
                cur_words.append(w)
                cur_tok += w_tok

        if cur_words:
            chunks.append(self._build_chunk(" ".join(cur_words)))
        return chunks

    def _build_chunk(self, txt: str) -> ChunkInfo:
        ids_len = self.count(txt)
        if ids_len > self.max_ctx:
            # ì–‘ìª½ ì²­í¬ ëª¨ë‘ ì²˜ë¦¬
            mid = len(txt) // 2
            left = txt[:mid]
            right = txt[mid:]
            left_chunk = self._build_chunk(left)
            right_chunk = self._build_chunk(right)
            return [left_chunk, right_chunk]  # ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜
        return ChunkInfo(txt, ids_len, hashlib.sha256(txt.encode()).hexdigest())


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. PDF ì¶”ì¶œê¸° (í˜ì´ì§€â€‘ë³‘ë ¬, threadâ€‘safe)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class AsyncPDFProcessor:
    def __init__(self, concurrency: int = 8):
        self.sem = asyncio.Semaphore(concurrency)
        self.doc_cache = {}  # PDF ë¬¸ì„œ ìºì‹œ

    async def extract(self, pdf_path: str) -> str:
        if pdf_path not in self.doc_cache:
            self.doc_cache[pdf_path] = fitz.open(pdf_path)
        doc = self.doc_cache[pdf_path]
        tasks = [self._extract_page(doc, i) for i in range(len(doc))]
        pages = await asyncio.gather(*tasks)
        return "\n".join(pages)

    async def _extract_page(self, doc, page_no: int) -> str:
        async with self.sem:
            loop = asyncio.get_running_loop()
            return await loop.run_in_executor(None,
                                              lambda: doc.load_page(page_no).get_text("text"))

    async def close(self):
        for doc in self.doc_cache.values():
            doc.close()
        self.doc_cache.clear()



# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. SQLite ìºì‹œ (í”„ë¡œì„¸ìŠ¤/ìŠ¤ë ˆë“œ ì•ˆì „)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class ResponseCache:
    def __init__(self, 
                 db_path: Path = Path("cache/response_cache.db"),
                 max_size: int = 10000,
                 ttl: timedelta = timedelta(days=7)
    ):
        self.db_path = db_path
        self.max_size = max_size
        self.ttl = ttl
        self.lock = asyncio.Lock()
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        # DB íŒŒì¼ ì‚­ì œí•˜ì§€ ì•Šê³  ìœ ì§€
        self._init_db()

    def _init_db(self):
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self.db = sqlite3.connect(self.db_path, check_same_thread=False)
        
        with self.db:
            self.db.execute("""
                CREATE TABLE IF NOT EXISTS cache (
                    key TEXT PRIMARY KEY,
                    val TEXT,
                    created_at TIMESTAMP DEFAULT (datetime('now')) NOT NULL
                )
            """)
            # ì¸ë±ìŠ¤ ì¶”ê°€
            self.db.execute("""
                CREATE INDEX IF NOT EXISTS idx_created_at ON cache(created_at)
            """)

    async def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì„ ì•ˆì „í•˜ê²Œ ì¢…ë£Œ"""
        async with self.lock:
            self.db.close()

    async def cleanup(self):
        """ì˜¤ë˜ëœ ìºì‹œ í•­ëª© ì •ë¦¬"""
        async with self.lock:
            with self.db:
                # TTL ì´ˆê³¼ í•­ëª© ì‚­ì œ
                self.db.execute(
                    "DELETE FROM cache WHERE created_at < datetime('now', ?)",
                    (f"-{self.ttl.days} days",)
                )

                # ìµœëŒ€ í¬ê¸° ì´ˆê³¼ì‹œ ì˜¤ë˜ëœ í•­ëª©ë¶€í„° ì‚­ì œ
                self.db.execute("""
                    DELETE FROM cache 
                    WHERE key IN (
                        SELECT key FROM cache 
                        ORDER BY created_at DESC 
                        LIMIT -1 OFFSET ?
                    )
                """, (self.max_size,))

    async def get(self, key: str) -> Optional[str]:
        await self.cleanup()  # ìºì‹œ ì •ë¦¬
        async with self.lock:
            cur = self.db.execute(
                "SELECT val FROM cache WHERE key=? AND created_at > datetime('now', ?)",
                (key, f"-{self.ttl.days} days")
            )
            row = cur.fetchone()
            return row[0] if row else None

    async def put(self, key: str, val: str):
        await self.cleanup()  # ìºì‹œ ì •ë¦¬
        async with self.lock:
            with self.db:
                self.db.execute(
                    "INSERT OR REPLACE INTO cache (key, val, created_at) VALUES (?, ?, CURRENT_TIMESTAMP)",
                    (key, val)
                )


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Triton HTTP/2 ìŠ¤íŠ¸ë¦¬ë° í´ë¼ì´ì–¸íŠ¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class TritonClient:
    def __init__(self, url: str, batch: int = 4, timeout: float = 120.0):
        self.url = url.rstrip("/")
        self.sem = asyncio.Semaphore(batch)
        self.cache = ResponseCache()
        self.timeout = timeout
        self._session = None
        self.batch = batch  # batch ì†ì„± ì¶”ê°€

    @property
    def session(self) -> httpx.AsyncClient:
        if self._session is None:
            self._session = httpx.AsyncClient(
                http2=True,
                timeout=self.timeout,
                limits=httpx.Limits(max_keepalive_connections=self.batch)
            )
        return self._session

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()

    async def close(self):
        if self._session:
            await self._session.aclose()
            self._session = None
        await self.cache.close()

    async def generate_stream(self, prompt: str, max_new_tokens: int = 200) -> AsyncIterator[str]:
        try:
            key = hashlib.sha256(f"{prompt}:{max_new_tokens}".encode()).hexdigest()
            
            try:
                cached = await self.cache.get(key)
                if cached is not None:
                    logger.debug(f"ìºì‹œ íˆíŠ¸: {key[:8]}...")
                    for part in cached.split("\n"):
                        if part:
                            yield part
                    return
            except Exception as e:
                logger.warning(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")

            async with self.sem:
                full = []
                for attempt in range(3):
                    try:
                        async with self.session.stream(
                            "POST",
                            self.url,
                            json={"text_input": prompt, "max_tokens": max_new_tokens}
                        ) as r:
                            r.raise_for_status()
                            async for line in r.aiter_lines():
                                if not line or line.isspace():
                                    continue
                                # event: í—¤ë” ì²˜ë¦¬ ì¶”ê°€
                                if line.startswith('event:'):
                                    continue

                                # SSE í˜•ì‹ ì²˜ë¦¬
                                if line.startswith('data: '):
                                    try:
                                        # 'data: ' ì œê±° í›„ JSON íŒŒì‹±
                                        json_str = line[6:].strip()
                                        data = json.loads(json_str)
                                        text = data.get("text_output", "")
                                        if text:
                                            full.append(text)
                                            yield text
                                    except json.JSONDecodeError as e:
                                        logger.debug(f"ì‘ë‹µ ë°ì´í„°: {line!r}")
                                        logger.warning(f"JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
                                        continue

                            if full:
                                await self.cache.put(key, "\n".join(full))
                            return

                    except (httpx.HTTPError, asyncio.TimeoutError) as e:
                        if attempt == 2:
                            logger.error(f"Triton ì„œë²„ ì˜¤ë¥˜: {e}")
                            yield "[Triton Error]"
                            return
                        await asyncio.sleep(0.5 * (attempt + 1))

        except Exception as e:
            logger.error(f"ì˜ˆê¸°ì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}", exc_info=True)
            yield "[System Error]"


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. ì „ì²´ íŒŒì´í”„ë¼ì¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class Pipeline:
    def __init__(self, model_path: str = "./", triton_url: str = "http://203.250.238.30:8888/v2/models/ensemble/generate_stream"):
        self.token_mgr = TokenManager(model_path)
        self.pdf_proc = AsyncPDFProcessor()
        self.triton = TritonClient(triton_url)
        
        # logging ì„¤ì • ì¶”ê°€
        logging.basicConfig(level=logging.WARNING)
        transformers_logger = logging.getLogger("transformers")
        transformers_logger.setLevel(logging.ERROR)  # transformers ê²½ê³  ì–µì œ

    async def process_document(self, pdf_path: str) -> AsyncIterator[Dict]:
        try:
            # 1) PDF â†’ í…ìŠ¤íŠ¸
            raw_text = await self.pdf_proc.extract(pdf_path)
            if not raw_text.strip():
                raise ValueError(f"PDF íŒŒì¼ '{pdf_path}'ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # 2) í…ìŠ¤íŠ¸ â†’ ì²­í¬
            chunks = self.token_mgr.create_chunks(raw_text)
            if not chunks:
                raise ValueError("í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

            queue: asyncio.Queue = asyncio.Queue()

            async def _producer(idx: int, ck: ChunkInfo):
                prompt = f"[Chunk {idx+1}/{len(chunks)}]\n{ck.text}"
                try:
                    async for part in self.triton.generate_stream(prompt):
                        if part and isinstance(part, str):
                            await queue.put({
                                "idx": idx,
                                "total": len(chunks),
                                "text": part.strip()  # ê³µë°± ì œê±°
                            })
                except Exception as e:
                    logger.error(f"ì²­í¬ {idx+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                finally:
                    await queue.put("__DONE__")

            producers = [asyncio.create_task(_producer(i, c)) for i, c in enumerate(chunks)]
            done_cnt = 0
            
            while done_cnt < len(producers):
                item = await queue.get()
                if item == "__DONE__":
                    done_cnt += 1
                elif isinstance(item, dict) and item.get("text"):  # ìœ íš¨í•œ í…ìŠ¤íŠ¸ë§Œ ì „ë‹¬
                    yield item

            await asyncio.gather(*producers)
            
        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            raise

    async def close(self):
        await self.triton.close()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 6. gracefulÂ shutdown helper
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def shutdown(sig, loop):
    print(f"\nâ¬‡ï¸Â Signal {sig.name} received; shutting downâ€¦")
    tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]
    for t in tasks:
        t.cancel()
    await asyncio.gather(*tasks, return_exceptions=True)
    loop.stop()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. ì‹¤í–‰ ì˜ˆì‹œ (CLI)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

async def main(pdf_path: str):
    pipeline = Pipeline()
    try:
        async for chunk in pipeline.process_document(pdf_path):
            print(f"â–¸Â [{chunk['idx']+1}/{chunk['total']}]Â {chunk['text']}")
    finally:
        await pipeline.close()

import asyncio
import logging
from pathlib import Path
from typing import Optional, Dict, Any, List

# ê¸°ì¡´ main3.py ëª¨ë“ˆ ì„í¬íŠ¸
from main3 import OptimizedPipeline
from semantic_search import SemanticSearchEngine, SEMANTIC_SEARCH_AVAILABLE

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("summary_pipeline.log", mode="a")
    ]
)
logger = logging.getLogger(__name__)

class EmbeddingExtractiveAbstractiveSummarizer:
    """ì„ë² ë”© ê¸°ë°˜ ì¶”ì¶œí˜• ì‚¬ì „ í•„í„°ë§ + ì¶”ìƒì  ìš”ì•½ íŒŒì´í”„ë¼ì¸"""

    def __init__(
        self,
        model_path: str = "./",
        triton_url: str = "http://203.250.238.30:8888/v2/models/ensemble/generate_stream",
        embedding_model: str = "sentence-transformers/all-MiniLM-L6-v2"
    ):
        # ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        self.semantic_engine = None
        if SEMANTIC_SEARCH_AVAILABLE:
            try:
                self.semantic_engine = SemanticSearchEngine(model_name=embedding_model)
                logger.info(f"ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì„±ê³µ: {embedding_model}")
            except ImportError as e:
                logger.warning(f"ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        else:
            logger.warning("sentence-transformers/faiss ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì˜ë¯¸ ê²€ìƒ‰ ë¹„í™œì„±í™”")

        # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ê¸°ì¡´ ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ì¬ì‚¬ìš©)
        self.pipeline = OptimizedPipeline(model_path, triton_url)

    async def __aenter__(self):
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        await self.close()

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.pipeline.close()

    async def extract_and_summarize(self, text: str, target_length: int = 200) -> Dict[str, Any]:
        """í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ í›„ ìš”ì•½ ìƒì„±"""
        if not text.strip():
            return {
                "error": "ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤",
                "success": False
            }

        # 1. ì„ë² ë”© ê¸°ë°˜ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ
        if self.semantic_engine is not None:
            try:
                logger.info("ì„ë² ë”© ê¸°ë°˜ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì¤‘...")
                # í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì¿¼ë¦¬
                extraction_query = "ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš” ì •ë³´ë¥¼ ìš”ì•½"
                # ê´€ë ¨ ë¬¸ì¥ë§Œ ì¶”ì¶œ (top_këŠ” ë¬¸ì„œ ê¸¸ì´ì— ë”°ë¼ ë™ì  ì¡°ì •)
                top_k = min(30, max(10, len(text) // 500))
                filtered_text = self.semantic_engine.extract_relevant_context(
                    extraction_query, text, top_k=top_k
                )

                if filtered_text.strip():
                    # ì••ì¶•ë¥  ê³„ì‚°
                    compression_ratio = len(filtered_text) / len(text)
                    logger.info(f"ì„ë² ë”© í•„í„°ë§: {len(text)} â†’ {len(filtered_text)} ë¬¸ì ({compression_ratio:.3f}ë°°)")

                    # í•„í„°ë§ ì„±ê³µ ì‹œ ì´ë¥¼ ì‚¬ìš©
                    extraction_success = True
                    processed_text = filtered_text
                else:
                    # í•„í„°ë§ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì‚¬ìš©
                    logger.warning("í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì‹¤íŒ¨, ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©")
                    extraction_success = False
                    processed_text = text
            except Exception as e:
                logger.error(f"ì„ë² ë”© ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                extraction_success = False
                processed_text = text
        else:
            # ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„ ì—†ìŒ
            extraction_success = False
            processed_text = text

        # 2. ìš”ì•½ ìƒì„±
        try:
            # ì²­í‚¹ ë° ìš”ì•½
            summary_result = await self.pipeline.summarizer.smart_chunking_summary(
                processed_text, target_length
            )

            # ê²°ê³¼ì— ì¶”ì¶œ ì •ë³´ ì¶”ê°€
            summary_result["extraction_applied"] = extraction_success
            if extraction_success:
                summary_result["extraction_stats"] = {
                    "original_length": len(text),
                    "filtered_length": len(processed_text),
                    "compression_ratio": len(processed_text) / len(text)
                }

            summary_result["success"] = True
            return summary_result

        except Exception as e:
            logger.error(f"ìš”ì•½ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "error": f"ìš”ì•½ ì²˜ë¦¬ ì˜¤ë¥˜: {e}",
                "success": False,
                "extraction_applied": extraction_success
            }

    async def process_pdf(self, pdf_path: str, target_length: int = 200) -> Dict[str, Any]:
        """PDF íŒŒì¼ ì²˜ë¦¬ - ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í™œìš©"""
        try:
            # íŒŒì¼ ì¡´ì¬ í™•ì¸
            path = Path(pdf_path)
            if not path.exists():
                return {
                    "error": f"PDF íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {pdf_path}",
                    "success": False
                }

            # ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í™œìš© (ì„ë² ë”© ê²€ìƒ‰ ìë™ ì ìš©)
            result = await self.pipeline.process_document_optimized(
                pdf_path, target_length
            )

            # ê²°ê³¼ì— í˜„ì¬ ë°©ì‹ í‘œì‹œ
            if result.get("success", False):
                result["approach"] = "Option A - Embedding-based extractive pre-filter + abstractive summary"

            return result

        except Exception as e:
            logger.error(f"PDF ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "error": f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}",
                "success": False
            }

async def main(pdf_path: str = "example3.pdf"):
    """ë©”ì¸ í•¨ìˆ˜ - íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print(f"ğŸ“„ PDF ì²˜ë¦¬ ì‹œì‘: {pdf_path} (ì„ë² ë”© ê¸°ë°˜ ì¶”ì¶œí˜• + ì¶”ìƒì  ìš”ì•½ ë°©ì‹)")

    async with EmbeddingExtractiveAbstractiveSummarizer() as summarizer:
        try:
            result = await summarizer.process_pdf(pdf_path)

            if result.get("success", False):
                print("\nâœ… ìµœì¢… ìš”ì•½ --------------------")
                print(result.get("final_summary", "ìš”ì•½ ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤."))

                # ì¶”ê°€ ì •ë³´ ì¶œë ¥
                stats = result.get("processing_stats", {})
                if stats:
                    semantic_used = "ì„ë² ë”© ê²€ìƒ‰ ì ìš©ë¨" if stats.get("semantic_search_used", False) else "ì„ë² ë”© ê²€ìƒ‰ ë¯¸ì ìš©"
                    print(f"\nğŸ“Š ì²˜ë¦¬ í†µê³„: ì²­í¬ {stats.get('chunks_created', 0)}ê°œ, "  
                          f"ì••ì¶•ë¥  {stats.get('compression_ratio', 0):.3f}, {semantic_used}")
            else:
                error_msg = result.get("error", "ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜")
                print(f"âŒ ì²˜ë¦¬ ì‹¤íŒ¨: {error_msg}")

        except Exception as e:
            print(f"âŒ ì˜ˆê¸°ì¹˜ ì•Šì€ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    # ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œê±° ì„¤ì •
    transformers_logger = logging.getLogger("transformers")
    transformers_logger.setLevel(logging.ERROR)  # transformers ê²½ê³  ì–µì œ

    # ìƒ˜í”Œ PDF ì²˜ë¦¬
    sample_pdf = "example3.pdf"

    # ì´ë²¤íŠ¸ ë£¨í”„ ì„¤ì • ë° ì‹¤í–‰
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

    try:
        loop.run_until_complete(main(sample_pdf))
    except KeyboardInterrupt:
        print("\nâ¬‡ï¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ìš”ì²­ë¨...")
        # ì‹¤í–‰ ì¤‘ì¸ ëª¨ë“  íƒœìŠ¤í¬ ì •ë¦¬
        pending = asyncio.all_tasks(loop)
        for task in pending:
            task.cancel()

        # ì·¨ì†Œëœ íƒœìŠ¤í¬ë“¤ì´ ì™„ë£Œë  ë•Œê¹Œì§€ ëŒ€ê¸°
        if pending:
            loop.run_until_complete(asyncio.gather(*pending, return_exceptions=True))
    finally:
        try:
            loop.close()
        except Exception as e:
            print(f"ë£¨í”„ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")
        print("âœ… ì™„ë£Œ")
if __name__ == "__main__":
    sample_pdf = "example3.pdf"

    # Windowsì—ì„œëŠ” ì‹œê·¸ë„ í•¸ë“¤ëŸ¬ ëŒ€ì‹  ì˜ˆì™¸ ì²˜ë¦¬ ì‚¬ìš©
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    try:
        loop.run_until_complete(main(sample_pdf))
    except KeyboardInterrupt:
        print("\nâ¬‡ï¸ í”„ë¡œê·¸ë¨ ì¢…ë£Œ ìš”ì²­ë¨...")
        tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]
        for t in tasks:
            t.cancel()
        loop.run_until_complete(asyncio.gather(*tasks, return_exceptions=True))
    finally:
        loop.close()
        print("âœ… ì™„ë£Œ")
import logging
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
import re
from src.models.data_models import SemanticChunk

logger = logging.getLogger(__name__)

try:
    from sentence_transformers import SentenceTransformer
    import faiss
    SEMANTIC_SEARCH_AVAILABLE = True
except ImportError:
    logger.warning("sentence-transformers ë˜ëŠ” faiss-cpuê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì˜ë¯¸ ê²€ìƒ‰ ê¸°ëŠ¥ì´ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")
    SEMANTIC_SEARCH_AVAILABLE = False

class SemanticSearchEngine:
    def __init__(self, model_name: str = 'sentence-transformers/all-MiniLM-L6-v2'):
        """ë¬¸ì¥ ì„ë² ë”© ë° ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”"""
        if not SEMANTIC_SEARCH_AVAILABLE:
            raise ImportError("ì˜ë¯¸ ê²€ìƒ‰ì„ ìœ„í•´ 'sentence-transformers' ë° 'faiss-cpu'ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”.")

        self.model = SentenceTransformer(model_name)
        self.dimension = self.model.get_sentence_embedding_dimension()
        # FAISS ì¸ë±ìŠ¤ ìƒì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•œ L2 ì •ê·œí™” ë²¡í„°ë¡œ ì„¤ì •)
        self.index = faiss.IndexFlatIP(self.dimension)  # ë‚´ì (IP) ì¸ë±ìŠ¤ - ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ìš©
        self.chunks = []
        logger.info(f"ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ: {model_name}, ì„ë² ë”© ì°¨ì›: {self.dimension}")

    def split_into_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        if not text.strip():
            return []

        # í•œêµ­ì–´/ì˜ì–´ ë¬¸ì¥ ê²½ê³„ íŒ¨í„´ ê°œì„ 
        # ë§ˆì¹¨í‘œ/ëŠë‚Œí‘œ/ë¬¼ìŒí‘œ ë‹¤ìŒì— ê³µë°± ë˜ëŠ” ì¤„ë°”ê¿ˆì´ ìˆëŠ” ê²½ìš° (ë” ìœ ì—°í•œ ë§¤ì¹­)
        pattern = r'(?<=[.!?])(?=[\s\n]|$)'  
        sentences = re.split(pattern, text)

        # ë¹ˆ ë¬¸ì¥ ì œê±° ë° ìµœì†Œ ê¸¸ì´ í•„í„°ë§ (ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ì€ ì œì™¸)
        MIN_SENT_LEN = 10  # ìµœì†Œ ë¬¸ì¥ ê¸¸ì´
        filtered = [s.strip() for s in sentences if len(s.strip()) > MIN_SENT_LEN]

        # ë¬¸ì¥ ìˆ˜ê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì¤„ë°”ê¿ˆ ê¸°ì¤€ìœ¼ë¡œ ë‹¤ì‹œ ë¶„í• 
        if len(filtered) <= 3 and len(text) > 500:
            paragraphs = [p.strip() for p in text.split('\n') if p.strip()]
            paragraphs = [p for p in paragraphs if len(p) > MIN_SENT_LEN]
            if len(paragraphs) > len(filtered):
                logger.info(f"ë¬¸ì¥ ë¶„í•  ê°œì„ : ë¬¸ì¥ ê²½ê³„ {len(filtered)}ê°œ â†’ ë‹¨ë½ ê¸°ì¤€ {len(paragraphs)}ê°œ")
                return paragraphs

        return filtered

    def preprocess_chunks(self, text: str) -> List[SemanticChunk]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ê³  ì„ë² ë”©"""
        sentences = self.split_into_sentences(text)
        if not sentences:
            return []

        # ë¬¸ì¥ ì„ë² ë”© ìƒì„±
        embeddings = self.model.encode(sentences, normalize_embeddings=True)

        # ì²­í¬ ê°ì²´ ìƒì„±
        chunks = []
        for i, (sent, emb) in enumerate(zip(sentences, embeddings)):
            chunks.append(SemanticChunk(text=sent, embedding=emb, index=i))

        # ì €ì¥
        self.chunks = chunks

        # FAISS ì¸ë±ìŠ¤ êµ¬ì¶•
        self.index = faiss.IndexFlatIP(self.dimension)
        self.index.add(np.vstack([c.embedding for c in chunks]))

        logger.info(f"ì„ë² ë”© ì²˜ë¦¬ ì™„ë£Œ: {len(chunks)} ë¬¸ì¥")
        return chunks

    def search(self, query: str, top_k: int = 15) -> List[SemanticChunk]:
        """ì§ˆì˜ì™€ ê°€ì¥ ìœ ì‚¬í•œ top_kê°œ ë¬¸ì¥ ê²€ìƒ‰"""
        query_embedding = self.model.encode([query], normalize_embeddings=True)

        # ê²€ìƒ‰ ìˆ˜í–‰
        scores, indices = self.index.search(query_embedding, min(top_k, len(self.chunks)))

        # ê²€ìƒ‰ ê²°ê³¼ êµ¬ì„±
        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.chunks):
                chunk = self.chunks[idx]
                chunk.score = float(score)
                results.append(chunk)

        # ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        results.sort(key=lambda x: x.score, reverse=True)
        return results

    def extract_relevant_context(self, query: str, text: str, top_k: int = 15, dedup_threshold: float = 0.85) -> str:
        """ì›ë¬¸ì—ì„œ ì§ˆì˜ì™€ ê´€ë ¨ëœ ìƒìœ„ Kê°œ ë¬¸ì¥ë§Œ ì¶”ì¶œ - MAX_INPUT_TOKENS ì œí•œ í•´ê²°ìš©

        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            text: ì›ë³¸ í…ìŠ¤íŠ¸
            top_k: ì¶”ì¶œí•  ìƒìœ„ ë¬¸ì¥ ìˆ˜
            dedup_threshold: ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•œ ìœ ì‚¬ë„ ì„ê³„ê°’ (ë†’ì„ìˆ˜ë¡ ë” ì—„ê²©í•œ ì¤‘ë³µ ì œê±°)
        """
        # ì›ë¬¸ì„ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• í•˜ê³  ì„ë² ë”©
        self.preprocess_chunks(text)

        if not self.chunks:
            logger.warning("ì¶”ì¶œí•  ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤. ì›ë³¸ í…ìŠ¤íŠ¸ ì¼ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return text[:min(len(text), 2000)]  # ì›ë³¸ ì¼ë¶€ë§Œ ë°˜í™˜

        # ì§ˆì˜ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ top_kê°œ ë¬¸ì¥ ê²€ìƒ‰ (ì¤‘ë³µ ì œê±° ê³ ë ¤í•´ 2ë°° ë” ê°€ì ¸ì˜´)
        relevant_chunks = self.search(query, top_k=min(top_k * 2, len(self.chunks)))

        if not relevant_chunks:
            logger.warning("ê´€ë ¨ ë¬¸ì¥ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ í…ìŠ¤íŠ¸ ì¼ë¶€ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return text[:min(len(text), 2000)]  # ì›ë³¸ ì¼ë¶€ë§Œ ë°˜í™˜

        # ìœ ì‚¬í•œ ë¬¸ì¥ ì¤‘ë³µ ì œê±°
        unique_chunks = []
        duplicate_count = 0

        # ì¤‘ë³µ ì œê±° ì „ì— ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬ (ì ìˆ˜ê°€ ë†’ì€ ë¬¸ì¥ ìš°ì„  ë³´ì¡´)
        relevant_chunks.sort(key=lambda x: x.score, reverse=True)

        # ì‹¤ì œ ì‚¬ìš©í•  ë¬¸ì¥ ìˆ˜ (ì›í•˜ëŠ” ì–‘ì˜ 2ë°°ê¹Œì§€ ìµœëŒ€ í™•ë³´)
        max_chunks_needed = min(top_k * 2, len(relevant_chunks))

        for chunk in relevant_chunks:
            # ì„ë² ë”© ì—†ëŠ” ì²­í¬ ê±´ë„ˆë›°ê¸°
            if chunk.embedding is None or np.isnan(chunk.embedding).any():
                logger.warning("ì„ë² ë”©ì´ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ì²­í¬ ë°œê²¬, ê±´ë„ˆëœ€")
                continue

            is_duplicate = False

            # ê¸°ì¡´ ì„ íƒëœ ì²­í¬ë“¤ê³¼ í˜„ì¬ ì²­í¬ì˜ ìœ ì‚¬ë„ ê³„ì‚°
            for selected in unique_chunks:
                try:
                    # ì„ë² ë”© ìœ íš¨ì„± í™•ì¸
                    if selected.embedding is None or np.isnan(selected.embedding).any():
                        continue

                    similarity = float(np.dot(chunk.embedding, selected.embedding))  # ì½”ì‚¬ì¸ ìœ ì‚¬ë„

                    # ìœ ì‚¬ë„ê°€ ì„ê³„ê°’ë³´ë‹¤ ë†’ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ì²˜ë¦¬
                    if similarity > dedup_threshold:
                        is_duplicate = True
                        duplicate_count += 1
                        break
                except Exception as e:
                    logger.warning(f"ìœ ì‚¬ë„ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ ë°œìƒ (ë¬´ì‹œ): {e}")
                    continue

            # ì¤‘ë³µì´ ì•„ë‹ˆë©´ ì¶”ê°€
            if not is_duplicate:
                unique_chunks.append(chunk)

            # ì¶©ë¶„í•œ ìˆ˜ì˜ ê³ ìœ  ì²­í¬ë¥¼ ì–»ì—ˆìœ¼ë©´ ì¢…ë£Œ
            if len(unique_chunks) >= max_chunks_needed:
                break

        # ì¶©ë¶„í•œ ì²­í¬ê°€ ì„ íƒë˜ì§€ ì•Šì•˜ìœ¼ë©´ ê²½ê³ 
        if not unique_chunks and relevant_chunks:
            logger.warning("ì¤‘ë³µ ì œê±° í›„ ì„ íƒëœ ì²­í¬ê°€ ì—†ìŒ. ì›ë³¸ ì²­í¬ ì‚¬ìš©")
            unique_chunks = relevant_chunks[:max_chunks_needed]

        if duplicate_count > 0:
            logger.info(f"ì¤‘ë³µ ì œê±°: {duplicate_count}ê°œ ìœ ì‚¬ ë¬¸ì¥ ì œê±°ë¨ (ì„ê³„ê°’: {dedup_threshold})")

        # ì›ë˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬ (ë¬¸ì„œ íë¦„ ìœ ì§€)
        unique_chunks.sort(key=lambda x: x.index)

        # ì¶”ê°€: ë†’ì€ ì ìˆ˜ì˜ ë¬¸ì¥ ìš°ì„  ì„ íƒ (ì¤‘ìš”ë„ì— ë”°ë¥¸ í•„í„°ë§ ê°•í™”)
        # ìŠ¤ì½”ì–´ê°€ 0.5 ì´ìƒì¸ ë¬¸ì¥ë§Œ ì„ íƒ (ì„ê³„ê°’ ì„¤ì •)
        high_score_chunks = [chunk for chunk in unique_chunks if chunk.score >= 0.5]

        # ë†’ì€ ì ìˆ˜ ë¬¸ì¥ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë˜ ê²°ê³¼ ì‚¬ìš©
        final_chunks = high_score_chunks if len(high_score_chunks) >= min(5, len(unique_chunks)//2) else unique_chunks

        # ì—°ê´€ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([chunk.text for chunk in final_chunks])

        extraction_rate = len(final_chunks) / len(self.chunks) * 100 if self.chunks else 0
        logger.info(f"ì§ˆì˜ '{query}'ì— ëŒ€í•´ {len(final_chunks)}ê°œ ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ (ì´ {len(self.chunks)}ê°œ ì¤‘, {extraction_rate:.1f}%)")

        # ì½˜ì†”ì— ì¤‘ë³µ ì œê±° ë° ë¬¸ì¥ ì„ íƒ ì •ë³´ ì¶œë ¥
        if duplicate_count > 0:
            print(f"ğŸ’¡ ì„ë² ë”© ì²˜ë¦¬: {duplicate_count}ê°œ ìœ ì‚¬ ë¬¸ì¥ ì œê±°ë¨, {len(final_chunks)}ê°œ ë¬¸ì¥ ì„ íƒ")

        # ì„ë² ë”© ì²˜ë¦¬ ì •ë³´ ìˆ˜ì§‘
        self.embedding_info = {
            "model_name": getattr(self.model, "name", "sentence-transformers/all-MiniLM-L6-v2"),
            "dimension": self.dimension,
            "sentences_count": len(self.chunks),
            "selected_count": len(final_chunks),
            "deduped_count": duplicate_count,
            "filtering_ratio": extraction_rate,
            "dedup_threshold": dedup_threshold
        }

        # ì½˜ì†”ì— ì„ë² ë”© ìƒì„¸ ì •ë³´ ì¶œë ¥
        print(f"ğŸ“Š ì„ë² ë”© ì²˜ë¦¬: ì´ {len(self.chunks)}ê°œ ë¬¸ì¥ì—ì„œ {len(final_chunks)}ê°œ ì„ íƒ ({extraction_rate:.1f}%)")

        return context

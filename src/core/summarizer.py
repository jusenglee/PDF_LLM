import logging
from typing import Dict, Any, List
from src.utils.token_manager import AdaptiveTokenManager
from src.core.triton_client import OptimizedTritonClient

logger = logging.getLogger(__name__)

class HierarchicalSummarizer:
    def __init__(self, token_mgr: AdaptiveTokenManager, triton_client: OptimizedTritonClient):
        self.token_mgr = token_mgr
        self.triton = triton_client
        self.semantic_engine = None

        # ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        try:
            from src.search.semantic_search import SemanticSearchEngine
            self.semantic_engine = SemanticSearchEngine()
            logger.info("âœ… ì„ë² ë”© ëª¨ë“ˆ(ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„) ì´ˆê¸°í™” ì„±ê³µ")
            print("âœ… ì„ë² ë”© ëª¨ë“ˆ ë¡œë“œë¨: ë¬¸ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ ê¸°ëŠ¥ í™œì„±í™”")
        except ImportError as e:
            logger.warning(f"âš ï¸ ì„ë² ë”© ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("âš ï¸ ì„ë² ë”© ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: ì „ì²´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ëª¨ë“œë¡œ ì „í™˜")
            print("ğŸ’¡ ì„ë² ë”© í™œì„±í™” ë°©ë²•: pip install sentence-transformers faiss-cpu")

    async def smart_chunking_summary(
        self, 
        text: str, 
        target_length: int = 200
    ) -> Dict[str, Any]:
        """ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ê¸°ë°˜ ê³„ì¸µì  ìš”ì•½"""

        chunks, allocation = self.token_mgr.create_adaptive_chunks(text, target_length)

        if len(chunks) == 1:
            # ë‹¨ì¼ ì²­í¬ëŠ” ì§ì ‘ ìš”ì•½
            prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_length}ì ì´ë‚´ë¡œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{chunks[0].text}"
            # í† í° ê¸¸ì´ í™•ì¸ (ë””ë²„ê¹…ìš©, í† í¬ë‚˜ì´ì €ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
            if hasattr(self.triton, 'tokenizer') and self.triton.tokenizer:
                try:
                    token_count = len(self.triton.tokenizer(prompt, add_special_tokens=False).input_ids)
                    if token_count > 2000:
                        logger.warning(f"ë‹¨ì¼ ì²­í¬ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {token_count} í† í° (ìë™ ì˜ë¦¼ ì˜ˆì •)")
                except Exception:
                    pass  # í† í¬ë‚˜ì´ì € ì˜¤ë¥˜ëŠ” ë¬´ì‹œ

            summary = await self.triton._generate_single_cached(prompt, target_length)

            return {
                "chunk_summaries": [summary],
                "final_summary": summary,
                "processing_method": "direct"
            }

        # ë‹¤ì¤‘ ì²­í¬ ì²˜ë¦¬
        chunk_prompts = []
        # ì²­í¬ë³„ ìš”ì•½ ê¸¸ì´ ê°ì†Œ (ë” ê°•í•œ ì••ì¶•ë¥  ìœ„í•´)
        chars_per_chunk = int(target_length * 0.7)  # ìš”ì•½ ê¸¸ì´ 30% ê°ì†Œ

        # ì„ë² ë”© ê¸°ë°˜ ë¬¸ì¥ ì¶”ì¶œ ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
        semantic_extraction_used = False
        if self.semantic_engine is not None:
            try:
                logger.info("ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰ ì ìš© ì¤‘...")
                filtered_chunks = []
                for i, chunk in enumerate(chunks):
                    # ê° ì²­í¬ì— ëŒ€í•´ 'ì²­í¬ ìš”ì•½' ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œ
                    extraction_query = f"ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš” ì •ë³´ë¥¼ ìš”ì•½"
                    # ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ ìˆ˜ ì¦ê°€ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ)
                    filtered_text = self.semantic_engine.extract_relevant_context(
                        extraction_query, chunk.text, top_k=20
                    )
                    # ì›ë³¸ ì²­í¬ ëŒ€ì‹  í•„í„°ë§ëœ ë¬¸ì¥ë“¤ë§Œ ì‚¬ìš©
                    if filtered_text.strip():
                        filtered_chunks.append({
                            'index': i,
                            'original_chunk': chunk,
                            'filtered_text': filtered_text,
                            'token_reduction': len(chunk.text) / len(filtered_text) if filtered_text else 1.0
                        })

                # í•„í„°ë§ëœ ì²­í¬ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ì‚¬ìš©
                if filtered_chunks:
                    semantic_extraction_used = True
                    avg_reduction = sum(c['token_reduction'] for c in filtered_chunks) / len(filtered_chunks)
                    logger.info(f"ì„ë² ë”© ê²€ìƒ‰ìœ¼ë¡œ í…ìŠ¤íŠ¸ {avg_reduction:.2f}ë°° ì¶•ì†Œ (í‰ê· )")

                    # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ë¡œ ìƒì„±í˜• ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
                    for fc in filtered_chunks:
                        prompt = f"""# í…ìŠ¤íŠ¸ ìš”ì•½ ì‘ì—…

        ## ì›ë³¸ í…ìŠ¤íŠ¸ (íŒŒíŠ¸ {fc['index']+1}/{len(chunks)}):
        {fc['filtered_text']}

        ## ìš”ì•½ ì§€ì¹¨:
        1. ìœ„ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ {chars_per_chunk}ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
        2. ì¤‘ìš”í•˜ì§€ ì•Šì€ ì„¸ë¶€ì‚¬í•­ì€ ê³¼ê°íˆ ìƒëµí•˜ì„¸ìš”.
        3. ì›ë¬¸ì˜ í•µì‹¬ ê°œë…ê³¼ ì£¼ìš” ì•„ì´ë””ì–´ë¥¼ ë³´ì¡´í•˜ì„¸ìš”.
        4. ìš”ì•½ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì—°ê²°ì„±ì„ ìœ ì§€í•˜ì„¸ìš”.
        5. ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

        ## ìš”ì•½ ê²°ê³¼:"""
                        chunk_prompts.append(prompt)
            except Exception as e:
                logger.error(f"ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                semantic_extraction_used = False

        # ì„ë² ë”© ê²€ìƒ‰ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ìƒì„±í˜• ìš”ì•½ ë°©ì‹ ì‚¬ìš©
        if not semantic_extraction_used:
            for i, chunk in enumerate(chunks):
                prompt = f"""# í…ìŠ¤íŠ¸ ìš”ì•½ ì‘ì—…

        ## ì›ë³¸ í…ìŠ¤íŠ¸ (íŒŒíŠ¸ {i+1}/{len(chunks)}):
        {chunk.text}

        ## ìš”ì•½ ì§€ì¹¨:
        1. ìœ„ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ {chars_per_chunk}ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
        2. ì¤‘ìš”í•˜ì§€ ì•Šì€ ì„¸ë¶€ì‚¬í•­ì€ ê³¼ê°íˆ ìƒëµí•˜ì„¸ìš”.
        3. ì›ë¬¸ì˜ í•µì‹¬ ê°œë…ê³¼ ì£¼ìš” ì•„ì´ë””ì–´ë¥¼ ë³´ì¡´í•˜ì„¸ìš”.
        4. ìš”ì•½ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì—°ê²°ì„±ì„ ìœ ì§€í•˜ì„¸ìš”.
        5. ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

        ## ìš”ì•½ ê²°ê³¼:"""
                chunk_prompts.append(prompt)

        # ë³‘ë ¬ ìš”ì•½ ìƒì„± - ë¬¸ì ìˆ˜ë¥¼ í† í° ìˆ˜ë¡œ ë³€í™˜ (í‰ê·  1.5ë°°)
        approx_tokens_per_chunk = max(40, int(chars_per_chunk * 0.67))
        chunk_summaries = await self.triton.generate_parallel_optimized(
            chunk_prompts, 
            max_new_tokens=max(20, approx_tokens_per_chunk)  # ìµœì†Œ 20í† í° ë³´ì¥
        )

        # ìµœì¢… í†µí•© ìš”ì•½
        combined_text = "\n\n".join([s for s in chunk_summaries if s and not s.startswith("[ì˜¤ë¥˜")])  # ì˜¤ë¥˜ ì‘ë‹µ í•„í„°ë§

        if not combined_text.strip():
            logger.warning("ëª¨ë“  ì²­í¬ ìš”ì•½ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì§ì ‘ ìš”ì•½ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            # ëª¨ë“  ì²­í¬ ìš”ì•½ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ê°„ë‹¨í•œ ìš”ì•½ ì¶”ì¶œ
            # ì„ë² ë”© ê¸°ë°˜ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì‹œë„
            if self.semantic_engine is not None:
                try:
                    # í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì¿¼ë¦¬
                    extraction_query = "ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš” ì •ë³´ë¥¼ ìš”ì•½"
                    # ìƒìœ„ 20ê°œ ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ
                    filtered_text = self.semantic_engine.extract_relevant_context(
                        extraction_query, text, top_k=20
                    )
                    if filtered_text.strip():
                        short_text = filtered_text
                        logger.info(f"ì„ë² ë”© í•„í„°ë§ìœ¼ë¡œ ìµœì¢… ìš”ì•½ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(filtered_text)} ë¬¸ì")
                    else:
                        short_text = text[:min(len(text), 2000)]  # ì›ë³¸ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
                except Exception as e:
                    logger.error(f"ì„ë² ë”© ê¸°ë°˜ ìµœì¢… í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                    short_text = text[:min(len(text), 2000)]  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
            else:
                short_text = text[:min(len(text), 2000)]  # ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„ ì—†ì„ ê²½ìš° ì›ë³¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©

            final_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_length}ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{short_text}"
        else:
            # ëª©í‘œ ìš”ì•½ ê¸¸ì´ ì¦ê°€ (ë” ìƒì„¸í•œ ìš”ì•½ì„ ìœ„í•´)
            enhanced_target_length = int(target_length * 1.5)  # 50% ëŠ˜ë¦° ìš”ì•½ ê¸¸ì´

            final_prompt = f"""# ìµœì¢… ë¬¸ì„œ ìš”ì•½ ìƒì„±

        ## ë¶€ë¶„ë³„ ìš”ì•½ ëª©ë¡:
        {combined_text}

        ## í†µí•© ìš”ì•½ ì§€ì¹¨:
        1. ìœ„ ë¶€ë¶„ë³„ ìš”ì•½ë“¤ì„ ì¢…í•©í•˜ì—¬ ë¬¸ì„œì˜ ì „ì²´ ë‚´ìš©ì„ ëŒ€í‘œí•˜ëŠ” ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
        2. ìš”ì•½ì€ {enhanced_target_length}ì ì´ë‚´ë¡œ ì‘ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        3. ìš”ì•½ì€ ë‹¤ìŒ êµ¬ì„±ìš”ì†Œë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤:
           - ë¬¸ì„œì˜ ì „ì²´ì ì¸ ì£¼ì œì™€ ëª©ì 
           - í•µì‹¬ ë…¼ì ê³¼ ì£¼ìš” ë‚´ìš©
           - ì¤‘ìš”í•œ ê²°ë¡ ì´ë‚˜ ì‹œì‚¬ì 
        4. ìš”ì•½ì€ ë…¼ë¦¬ì  íë¦„ì„ ìœ ì§€í•˜ê³ , ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
        5. ëª¨ë“  ìš”ì•½ì€ ì›ë³¸ ë¬¸ì„œì˜ ë‚´ìš©ì— ì¶©ì‹¤í•´ì•¼ í•©ë‹ˆë‹¤.
        6. ê° ë¶€ë¶„ ìš”ì•½ì—ì„œ í•µì‹¬ì ì¸ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ í†µí•©í•˜ì„¸ìš”.

        ## ìµœì¢… ìš”ì•½:"""

            # ìµœì¢… ìš”ì•½ì— ì¶©ë¶„í•œ í† í° í• ë‹¹ (í•œê¸€ ë¬¸ì:í† í° ë¹„ìœ¨ ê³ ë ¤)
            approx_tokens = max(200, int(enhanced_target_length * 3))  # í† í° í• ë‹¹ëŸ‰ í¬ê²Œ ì¦ê°€
            final_summary = await self.triton._generate_single_cached(final_prompt, approx_tokens)

        # ì²˜ë¦¬ ë°©ë²• ê²°ì •
        processing_method = "hierarchical"
        if semantic_extraction_used:
            processing_method = "hierarchical_with_embedding_filter"

        return {
            "chunk_summaries": chunk_summaries,
            "final_summary": final_summary,
            "processing_method": processing_method,
            "semantic_extraction_used": semantic_extraction_used
        }

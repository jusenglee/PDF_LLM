import logging
import time
import os
import time
from typing import Dict, Any, List
from src.utils.token_manager import AdaptiveTokenManager
from src.core.triton_client import OptimizedTritonClient

logger = logging.getLogger(__name__)

class HierarchicalSummarizer:
    def __init__(self, token_mgr: AdaptiveTokenManager, triton_client: OptimizedTritonClient):
        self.token_mgr = token_mgr
        self.triton = triton_client
        self.semantic_engine = None

        # ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        try:
            from src.search.semantic_search import SemanticSearchEngine
            self.semantic_engine = SemanticSearchEngine()
            logger.info("âœ… ì„ë² ë”© ëª¨ë“ˆ(ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„) ì´ˆê¸°í™” ì„±ê³µ")
            print("âœ… ì„ë² ë”© ëª¨ë“ˆ ë¡œë“œë¨: ë¬¸ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ ê¸°ëŠ¥ í™œì„±í™”")
        except ImportError as e:
            logger.warning(f"âš ï¸ ì„ë² ë”© ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("âš ï¸ ì„ë² ë”© ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: ì „ì²´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ëª¨ë“œë¡œ ì „í™˜")
            print("ğŸ’¡ ì„ë² ë”© í™œì„±í™” ë°©ë²•: pip install sentence-transformers faiss-cpu")

    async def smart_chunking_summary(
        self, 
        text: str, 
        target_length: int = 200
    ) -> Dict[str, Any]:
        """ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ê¸°ë°˜ ê³„ì¸µì  ìš”ì•½"""

        chunks, allocation = self.token_mgr.create_adaptive_chunks(text, target_length)

        if len(chunks) == 1:
            # ë‹¨ì¼ ì²­í¬ëŠ” ì§ì ‘ ìš”ì•½
            prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_length}ì ì´ë‚´ë¡œ ìµœëŒ€í•œ ì§§ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{chunks[0].text}"
            # í† í° ê¸¸ì´ í™•ì¸ (ë””ë²„ê¹…ìš©, í† í¬ë‚˜ì´ì €ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
            if hasattr(self.triton, 'tokenizer') and self.triton.tokenizer:
                try:
                    token_count = len(self.triton.tokenizer(prompt, add_special_tokens=False).input_ids)
                    if token_count > 2000:
                        logger.warning(f"ë‹¨ì¼ ì²­í¬ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {token_count} í† í° (ìë™ ì˜ë¦¼ ì˜ˆì •)")
                except Exception:
                    pass  # í† í¬ë‚˜ì´ì € ì˜¤ë¥˜ëŠ” ë¬´ì‹œ

            summary = await self.triton._generate_single_cached(prompt, target_length)

            return {
                "chunk_summaries": [summary],
                "final_summary": summary,
                "processing_method": "direct"
            }

        # ë‹¤ì¤‘ ì²­í¬ ì²˜ë¦¬
        chunk_prompts = []
        # ì²­í¬ë³„ ìš”ì•½ ê¸¸ì´ ê°ì†Œ (ë” ê°•í•œ ì••ì¶•ë¥  ìœ„í•´)
        chars_per_chunk = int(target_length * 0.7)  # ìš”ì•½ ê¸¸ì´ 30% ê°ì†Œ

        # ì„ë² ë”© ê¸°ë°˜ ë¬¸ì¥ ì¶”ì¶œ ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
        semantic_extraction_used = False
        if self.semantic_engine is not None:
            try:
                logger.info("ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰ ì ìš© ì¤‘...")
                filtered_chunks = []
                for i, chunk in enumerate(chunks):
                    # ê° ì²­í¬ì— ëŒ€í•´ 'ì²­í¬ ìš”ì•½' ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œ
                    extraction_query = f"ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš” ì •ë³´ë¥¼ ìµœëŒ€í•œ ì§§ê²Œ ìš”ì•½"
                    # ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ ìˆ˜ ì¦ê°€ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ) ë° ì¤‘ë³µ ì œê±° ì„ê³„ê°’ ì„¤ì •
                    # 0.85 ì„ê³„ê°’ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ì¥ ì¤‘ë³µ ì œê±° í™œì„±í™”
                    filtered_text = self.semantic_engine.extract_relevant_context(
                        extraction_query, chunk.text, top_k=20, dedup_threshold=0.85
                    )
                    # ì›ë³¸ ì²­í¬ ëŒ€ì‹  í•„í„°ë§ëœ ë¬¸ì¥ë“¤ë§Œ ì‚¬ìš©
                    if filtered_text.strip():
                        filtered_chunks.append({
                            'index': i,
                            'original_chunk': chunk,
                            'filtered_text': filtered_text,
                            'token_reduction': len(chunk.text) / len(filtered_text) if filtered_text else 1.0
                        })

                # í•„í„°ë§ëœ ì²­í¬ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ì‚¬ìš©
                if filtered_chunks:
                    semantic_extraction_used = True
                    avg_reduction = sum(c['token_reduction'] for c in filtered_chunks) / len(filtered_chunks)
                    logger.info(f"ì„ë² ë”© ê²€ìƒ‰ìœ¼ë¡œ í…ìŠ¤íŠ¸ {avg_reduction:.2f}ë°° ì¶•ì†Œ (í‰ê· )")

                    # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ë¡œ ìƒì„±í˜• ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
                    for fc in filtered_chunks:
                        prompt = f"""# í…ìŠ¤íŠ¸ ìš”ì•½ ì‘ì—…

        ## ì›ë³¸ í…ìŠ¤íŠ¸ (íŒŒíŠ¸ {fc['index']+1}/{len(chunks)}):
        {fc['filtered_text']}

        ## ìš”ì•½ ì§€ì¹¨:
        1. ìœ„ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ {chars_per_chunk}ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
        2. ì¤‘ìš”í•˜ì§€ ì•Šì€ ì„¸ë¶€ì‚¬í•­ì€ ê³¼ê°íˆ ìƒëµí•˜ì„¸ìš”.
        3. ì›ë¬¸ì˜ í•µì‹¬ ê°œë…ê³¼ ì£¼ìš” ì•„ì´ë””ì–´ë¥¼ ë³´ì¡´í•˜ì„¸ìš”.
        4. ìš”ì•½ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì—°ê²°ì„±ì„ ìœ ì§€í•˜ì„¸ìš”.
        5. ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

        ## ìš”ì•½ ê²°ê³¼:"""
                        chunk_prompts.append(prompt)
            except Exception as e:
                logger.error(f"ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                semantic_extraction_used = False

        # ì„ë² ë”© ê²€ìƒ‰ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ìƒì„±í˜• ìš”ì•½ ë°©ì‹ ì‚¬ìš©
        if not semantic_extraction_used:
            for i, chunk in enumerate(chunks):
                prompt = f"""# í…ìŠ¤íŠ¸ ìš”ì•½ ì‘ì—…

        ## ì›ë³¸ í…ìŠ¤íŠ¸ (íŒŒíŠ¸ {i+1}/{len(chunks)}):
        {chunk.text}

        ## ìš”ì•½ ì§€ì¹¨:
        1. ìœ„ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ {chars_per_chunk}ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•˜ì„¸ìš”.
        2. ì¤‘ìš”í•˜ì§€ ì•Šì€ ì„¸ë¶€ì‚¬í•­ì€ ê³¼ê°íˆ ìƒëµí•˜ì„¸ìš”.
        3. ì›ë¬¸ì˜ í•µì‹¬ ê°œë…ê³¼ ì£¼ìš” ì•„ì´ë””ì–´ë¥¼ ë³´ì¡´í•˜ì„¸ìš”.
        4. ìš”ì•½ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì—°ê²°ì„±ì„ ìœ ì§€í•˜ì„¸ìš”.
        5. ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.

        ## ìš”ì•½ ê²°ê³¼:"""
                chunk_prompts.append(prompt)

        # ë³‘ë ¬ ìš”ì•½ ìƒì„± - ë¬¸ì ìˆ˜ë¥¼ í† í° ìˆ˜ë¡œ ë³€í™˜ (í•œêµ­ì–´ì˜ ê²½ìš° ë” ë†’ì€ ë¹„ìœ¨)
        approx_tokens_per_chunk = max(60, int(chars_per_chunk * 1.0))
        chunk_summaries = await self.triton.generate_parallel_optimized(
            chunk_prompts, 
            max_new_tokens=max(40, approx_tokens_per_chunk)  # ìµœì†Œ 40í† í° ë³´ì¥
        )

        # ìµœì¢… í†µí•© ìš”ì•½
        combined_text = "\n\n".join([s for s in chunk_summaries if s and not s.startswith("[ì˜¤ë¥˜")])  # ì˜¤ë¥˜ ì‘ë‹µ í•„í„°ë§

        if not combined_text.strip():
            logger.warning("ëª¨ë“  ì²­í¬ ìš”ì•½ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì§ì ‘ ìš”ì•½ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            # ëª¨ë“  ì²­í¬ ìš”ì•½ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ê°„ë‹¨í•œ ìš”ì•½ ì¶”ì¶œ
            # ì„ë² ë”© ê¸°ë°˜ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì‹œë„
            if self.semantic_engine is not None:
                try:
                    # í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì¿¼ë¦¬
                    extraction_query = "ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš” ì •ë³´ë¥¼ ìš”ì•½"
                    # ìƒìœ„ 20ê°œ ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ (ì¤‘ë³µ ì œê±° ì ìš©)
                    filtered_text = self.semantic_engine.extract_relevant_context(
                        extraction_query, text, top_k=20, dedup_threshold=0.85
                    )
                    if filtered_text.strip():
                        short_text = filtered_text
                        logger.info(f"ì„ë² ë”© í•„í„°ë§ìœ¼ë¡œ ìµœì¢… ìš”ì•½ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(filtered_text)} ë¬¸ì")
                    else:
                        short_text = text[:min(len(text), 2000)]  # ì›ë³¸ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
                except Exception as e:
                    logger.error(f"ì„ë² ë”© ê¸°ë°˜ ìµœì¢… í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                    short_text = text[:min(len(text), 2000)]  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
            else:
                short_text = text[:min(len(text), 2000)]  # ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„ ì—†ì„ ê²½ìš° ì›ë³¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©

            final_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_length}ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{short_text}"
        else:
            # ëª©í‘œ ìš”ì•½ ê¸¸ì´ ì¦ê°€ (ë” ìƒì„¸í•œ ìš”ì•½ì„ ìœ„í•´)
            enhanced_target_length = int(target_length * 1.5)  # 50% ëŠ˜ë¦° ìš”ì•½ ê¸¸ì´

            final_prompt = f"""# ë¬¸ì„œ ìš”ì•½ ìƒì„±

        ## ìš”ì•½ ì‘ì—…:
        ë‹¤ìŒì€ ê¸´ ë¬¸ì„œì˜ íŒŒíŠ¸ë³„ ìš”ì•½ì…ë‹ˆë‹¤. ì´ ìš”ì•½ë“¤ì„ í†µí•©í•˜ì—¬ ì „ì²´ ë‚´ìš©ì„ ë‹´ì€ ìµœì¢… ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

        ## íŒŒíŠ¸ë³„ ìš”ì•½:
        {combined_text}

        ## ìš”ì•½ ì¡°ê±´:
        - ìš”ì•½ ê¸¸ì´: {enhanced_target_length}ì ì´ë‚´
        - ë¬¸ì„œì˜ ì£¼ì œ, í•µì‹¬ ë…¼ì , ì£¼ìš” ë‚´ìš©ì„ í¬í•¨í•  ê²ƒ
        - ë…¼ë¦¬ì  íë¦„ì„ ìœ ì§€í•˜ê³  ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ
        - ê° íŒŒíŠ¸ì˜ í•µì‹¬ë§Œ ì¶”ì¶œí•˜ì—¬ í†µí•©í•  ê²ƒ

        ## ìµœì¢… ìš”ì•½:"""

            # ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ë¡œê¹…
            try:
                from pathlib import Path
                from config.settings import LOG_DIR

                # ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸
                prompt_log_dir = LOG_DIR / "prompts"
                os.makedirs(prompt_log_dir, exist_ok=True)

                # íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                final_log_file = prompt_log_dir / f"final_prompt_{timestamp}.log"

                # ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ì €ì¥
                with open(final_log_file, "w", encoding="utf-8") as f:
                    f.write("==== ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ====\n\n")
                    f.write(final_prompt)

                logger.info(f"ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ë¡œê·¸ ì €ì¥ë¨: {final_log_file}")
                print(f"â„¹ï¸ ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ë¡œê·¸ ì €ì¥ë¨: {final_log_file}")
            except Exception as e:
                logger.warning(f"ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ë¡œê¹… ì‹¤íŒ¨: {e}")

            # ìµœì¢… ìš”ì•½ì— ì¶©ë¶„í•œ í† í° í• ë‹¹ (í•œê¸€ ë¬¸ì:í† í° ë¹„ìœ¨ ê³ ë ¤)
            approx_tokens = max(300, int(enhanced_target_length * 4))  # í† í° í• ë‹¹ëŸ‰ ë” í¬ê²Œ ì¦ê°€

            print(f"\nğŸ”„ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘... (ìµœëŒ€ {approx_tokens} í† í° í• ë‹¹)")
            final_start_time = time.time()
            # í† í° ì •ë³´ ë¡œê¹… í™œì„±í™”
            final_summary = await self.triton._generate_single_cached(final_prompt, approx_tokens, log_tokens=True, log_prompt=True)
            final_time = time.time() - final_start_time
            print(f"âœ… ìµœì¢… ìš”ì•½ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {final_time:.2f}ì´ˆ)")

        # ì²˜ë¦¬ ë°©ë²• ê²°ì •
        processing_method = "hierarchical"
        if semantic_extraction_used:
            processing_method = "hierarchical_with_embedding_filter"

        return {
            "chunk_summaries": chunk_summaries,
            "final_summary": final_summary,
            "processing_method": processing_method,
            "semantic_extraction_used": semantic_extraction_used
        }

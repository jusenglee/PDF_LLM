import logging
import time
import os
from typing import Dict, Any, List
from src.utils.token_manager import AdaptiveTokenManager
from src.core.triton_client import OptimizedTritonClient

logger = logging.getLogger(__name__)

MAX_SELECTED_CHUNKS = 12  # ì„ë² ë”© ê¸°ë°˜ ì¤‘ìš”ë„ ì„ ë³„ ì‹œ ìµœëŒ€ ì‚¬ìš©í•  ì²­í¬ ìˆ˜


class HierarchicalSummarizer:
    def __init__(
        self, token_mgr: AdaptiveTokenManager, triton_client: OptimizedTritonClient
    ):
        self.token_mgr = token_mgr
        self.triton = triton_client
        self.semantic_engine = None

        # ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
        try:
            from src.search.semantic_search import SemanticSearchEngine

            self.semantic_engine = SemanticSearchEngine()
            logger.info("âœ… ì„ë² ë”© ëª¨ë“ˆ(ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„) ì´ˆê¸°í™” ì„±ê³µ")
            print("âœ… ì„ë² ë”© ëª¨ë“ˆ ë¡œë“œë¨: ë¬¸ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ ê¸°ëŠ¥ í™œì„±í™”")
        except ImportError as e:
            logger.warning(f"âš ï¸ ì„ë² ë”© ëª¨ë“ˆ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("âš ï¸ ì„ë² ë”© ëª¨ë“ˆ ë¡œë“œ ì‹¤íŒ¨: ì „ì²´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ëª¨ë“œë¡œ ì „í™˜")
            print("ğŸ’¡ ì„ë² ë”© í™œì„±í™” ë°©ë²•: pip install sentence-transformers faiss-cpu")

    def _select_top_chunks_by_embedding(
        self, chunks: List, query: str, top_k: int = MAX_SELECTED_CHUNKS
    ) -> (List, List[int]):
        """ì„ë² ë”© ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ ì²­í¬ ì„ ë³„"""
        if self.semantic_engine is None:
            return chunks, list(range(len(chunks)))

        try:
            model = self.semantic_engine.model
            query_emb = model.encode([query], normalize_embeddings=True)[0]
            chunk_embs = model.encode(
                [c.text for c in chunks], normalize_embeddings=True
            )
            scores = [float((emb * query_emb).sum()) for emb in chunk_embs]
            ranked = sorted(enumerate(chunks), key=lambda x: scores[x[0]], reverse=True)
            selected = ranked[: min(top_k, len(ranked))]
            selected_indices = sorted([idx for idx, _ in selected])
            selected_chunks = [chunks[i] for i in selected_indices]
            logger.info(
                f"ì„ë² ë”© ì¤‘ìš”ë„ ìˆœ ì²­í¬ ì„ ë³„: {len(selected_chunks)}/{len(chunks)}ê°œ ì„ íƒ"
            )
            print(
                f"ğŸ’¡ ì¤‘ìš”ë„ ê¸°ë°˜ ìƒìœ„ {len(selected_chunks)}ê°œ ì²­í¬ ì„ íƒ (ì´ {len(chunks)}ê°œ ì¤‘)"
            )
            return selected_chunks, selected_indices
        except Exception as e:
            logger.warning(f"ì²­í¬ ì¤‘ìš”ë„ ì„ ë³„ ì‹¤íŒ¨: {e}")
            return chunks, list(range(len(chunks)))

    async def smart_chunking_summary(
        self, text: str, target_length: int = 200
    ) -> Dict[str, Any]:
        """ìŠ¤ë§ˆíŠ¸ ì²­í‚¹ ê¸°ë°˜ ê³„ì¸µì  ìš”ì•½"""

        chunks, allocation = self.token_mgr.create_adaptive_chunks(text, target_length)

        original_chunk_count = len(chunks)
        selected_indices = list(range(original_chunk_count))

        # ì²­í¬ ìˆ˜ê°€ ë§ìœ¼ë©´ ì„ë² ë”©ì„ í™œìš©í•´ ì¤‘ìš”ë„ê°€ ë†’ì€ ì²­í¬ë§Œ ì„ ë³„
        if (
            self.semantic_engine is not None
            and original_chunk_count > MAX_SELECTED_CHUNKS
        ):
            query = "ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš” ì •ë³´ë¥¼ ìš”ì•½"
            chunks, selected_indices = self._select_top_chunks_by_embedding(
                chunks, query, MAX_SELECTED_CHUNKS
            )

        if len(chunks) == 1:
            # ë‹¨ì¼ ì²­í¬ëŠ” ì§ì ‘ ìš”ì•½
            prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_length}ì ì´ë‚´ë¡œ ìµœëŒ€í•œ ì§§ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{chunks[0].text}"
            # í† í° ê¸¸ì´ í™•ì¸ (ë””ë²„ê¹…ìš©, í† í¬ë‚˜ì´ì €ê°€ ìˆëŠ” ê²½ìš°ë§Œ)
            if hasattr(self.triton, "tokenizer") and self.triton.tokenizer:
                try:
                    token_count = len(
                        self.triton.tokenizer(
                            prompt, add_special_tokens=False
                        ).input_ids
                    )
                    if token_count > 2000:
                        logger.warning(
                            f"ë‹¨ì¼ ì²­í¬ í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {token_count} í† í° (ìë™ ì˜ë¦¼ ì˜ˆì •)"
                        )
                except Exception:
                    pass  # í† í¬ë‚˜ì´ì € ì˜¤ë¥˜ëŠ” ë¬´ì‹œ

            summary = await self.triton._generate_single_cached(prompt, target_length)

            return {
                "chunk_summaries": [summary],
                "final_summary": summary,
                "processing_method": "direct",
            }

        # ë‹¤ì¤‘ ì²­í¬ ì²˜ë¦¬
        chunk_prompts = []
        # ì²­í¬ë³„ ìš”ì•½ ê¸¸ì´ë¥¼ ë™ì ìœ¼ë¡œ í• ë‹¹
        base_chars_per_chunk = int(target_length * 0.7)  # ê¸°ë³¸ ìš”ì•½ ê¸¸ì´ (30% ê°ì†Œ)

        # ì²­í¬ë³„ ë³µì¡ë„ì— ë”°ë¥¸ ë™ì  í† í° í• ë‹¹ ê³„ì‚° í•¨ìˆ˜
        def calculate_dynamic_token_allocation(chunks):
            """ì²­í¬ ê¸¸ì´ì™€ ë³µì¡ë„ì— ë”°ë¼ ë™ì ìœ¼ë¡œ í† í° í• ë‹¹"""
            # ì²­í¬ ê¸¸ì´ ì •ê·œí™” (ìƒëŒ€ì  ê¸¸ì´ ê³„ì‚°)
            total_tokens = sum(chunk.token_count for chunk in chunks)
            avg_tokens = total_tokens / len(chunks) if chunks else base_chars_per_chunk

            # ê° ì²­í¬ë³„ ìƒëŒ€ì  í† í° í• ë‹¹ëŸ‰ ê³„ì‚°
            allocations = {}

            # ë³µì¡ë„ ì§€í‘œ: ë¬¸ì¥ ë‹¹ í‰ê·  ë‹¨ì–´ ìˆ˜ë¡œ ì¶”ì •
            complexity_scores = {}

            # ì²­í¬ë³„ ë¬¸ì¥ ìˆ˜ ê³„ì‚°
            for i, chunk in enumerate(chunks):
                text = chunk.text
                sentences = [s.strip() for s in text.split(".") if s.strip()]
                words = text.split()

                # ë¬¸ì¥ ìˆ˜ì™€ ë‹¨ì–´ ìˆ˜ ê³„ì‚°
                sent_count = max(1, len(sentences))
                words_count = len(words)

                # ë¬¸ì¥ë‹¹ í‰ê·  ë‹¨ì–´ ìˆ˜ë¡œ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
                complexity = words_count / sent_count
                complexity_scores[i] = complexity

                # ì²­í¬ ê¸¸ì´ ì •ê·œí™” (ìƒëŒ€ì  í¬ê¸°)
                size_factor = chunk.token_count / avg_tokens

                # ë³µì¡ë„ì™€ í¬ê¸°ë¥¼ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜ ê³„ì‚°
                allocations[i] = {
                    "size_factor": size_factor,
                    "complexity": complexity,
                    "token_count": chunk.token_count,
                }

            # ë³µì¡ë„ ì ìˆ˜ ì •ê·œí™”
            if complexity_scores:
                avg_complexity = sum(complexity_scores.values()) / len(
                    complexity_scores
                )
                max_complexity = max(complexity_scores.values())
                min_complexity = min(complexity_scores.values())
                complexity_range = max(0.5, max_complexity - min_complexity)

                # ì •ê·œí™”ëœ ë³µì¡ë„ ì ìˆ˜ ì¶”ê°€
                for i in allocations:
                    norm_complexity = (
                        (complexity_scores[i] - min_complexity) / complexity_range
                        if complexity_range > 0
                        else 0.5
                    )
                    allocations[i]["norm_complexity"] = max(
                        0.7, min(1.3, 0.7 + norm_complexity)
                    )

            # ìµœì¢… í• ë‹¹ ê³„ì‚°
            final_allocations = {}
            for i, alloc in allocations.items():
                # ê¸°ë³¸ í• ë‹¹ (ì²­í¬ í¬ê¸° ê¸°ë°˜)
                base_alloc = base_chars_per_chunk * alloc.get("size_factor", 1.0)

                # ë³µì¡ë„ ê¸°ë°˜ ì¡°ì •
                complexity_factor = alloc.get("norm_complexity", 1.0)

                # ìµœì¢… í• ë‹¹ (ìµœì†Œ/ìµœëŒ€ ë²”ìœ„ ì œí•œ)
                final_alloc = int(base_alloc * complexity_factor)

                # ë„ˆë¬´ ì ê±°ë‚˜ ë§ì€ í• ë‹¹ ë°©ì§€
                final_alloc = max(
                    int(base_chars_per_chunk * 0.7),
                    min(int(base_chars_per_chunk * 1.5), final_alloc),
                )

                final_allocations[i] = final_alloc

            logger.info(f"ë™ì  í† í° í• ë‹¹: {final_allocations}")
            print(f"ğŸ” ì²­í¬ë³„ ë™ì  í† í° í• ë‹¹ ê³„ì‚° ì™„ë£Œ")
            return final_allocations

        # ë™ì  í† í° í• ë‹¹ ê³„ì‚°
        token_allocations = calculate_dynamic_token_allocation(chunks)

        # ì²­í¬ ìš”ì•½ì— ì‚¬ìš©í•  ê¸°ë³¸ ë¬¸ì ìˆ˜
        chars_per_chunk = base_chars_per_chunk

        # ì„ë² ë”© ê¸°ë°˜ ë¬¸ì¥ ì¶”ì¶œ ì‚¬ìš© (ê°€ëŠ¥í•œ ê²½ìš°)
        semantic_extraction_used = False
        if self.semantic_engine is not None:
            try:
                logger.info("ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰ ì ìš© ì¤‘...")
                filtered_chunks = []
                for i, chunk in enumerate(chunks):
                    # ê° ì²­í¬ì— ëŒ€í•´ 'ì²­í¬ ìš”ì•½' ì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í•µì‹¬ ë¬¸ì¥ë§Œ ì¶”ì¶œ
                    extraction_query = (
                        f"ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš” ì •ë³´ë¥¼ ìµœëŒ€í•œ ì§§ê²Œ ìš”ì•½"
                    )
                    # ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ ìˆ˜ ì¦ê°€ (ë” ë§ì€ ì»¨í…ìŠ¤íŠ¸ ì œê³µ) ë° ì¤‘ë³µ ì œê±° ì„ê³„ê°’ ì„¤ì •
                    # 0.85 ì„ê³„ê°’ìœ¼ë¡œ ìœ ì‚¬ ë¬¸ì¥ ì¤‘ë³µ ì œê±° í™œì„±í™”
                    filtered_text = self.semantic_engine.extract_relevant_context(
                        extraction_query, chunk.text, top_k=20, dedup_threshold=0.85
                    )
                    # ì›ë³¸ ì²­í¬ ëŒ€ì‹  í•„í„°ë§ëœ ë¬¸ì¥ë“¤ë§Œ ì‚¬ìš©
                    if filtered_text.strip():
                        filtered_chunks.append(
                            {
                                "index": i,
                                "original_chunk": chunk,
                                "filtered_text": filtered_text,
                                "token_reduction": (
                                    len(chunk.text) / len(filtered_text)
                                    if filtered_text
                                    else 1.0
                                ),
                            }
                        )

                # í•„í„°ë§ëœ ì²­í¬ê°€ ìˆìœ¼ë©´ ì´ë¥¼ ì‚¬ìš©
                if filtered_chunks:
                    semantic_extraction_used = True
                    avg_reduction = sum(
                        c["token_reduction"] for c in filtered_chunks
                    ) / len(filtered_chunks)
                    logger.info(
                        f"ì„ë² ë”© ê²€ìƒ‰ìœ¼ë¡œ í…ìŠ¤íŠ¸ {avg_reduction:.2f}ë°° ì¶•ì†Œ (í‰ê· )"
                    )

                    # í•„í„°ë§ëœ í…ìŠ¤íŠ¸ë¡œ ìƒì„±í˜• ìš”ì•½ í”„ë¡¬í”„íŠ¸ ìƒì„±
                    for fc in filtered_chunks:
                        # í•´ë‹¹ ì²­í¬ì— ëŒ€í•œ ë™ì  í• ë‹¹ í¬ê¸° ì‚¬ìš©
                        idx = fc["index"]
                        dynamic_length = token_allocations.get(idx, chars_per_chunk)

                        prompt = f"""# í…ìŠ¤íŠ¸ ìš”ì•½ ì‘ì—…

        ## ì›ë³¸ í…ìŠ¤íŠ¸ (íŒŒíŠ¸ {fc['index']+1}/{len(chunks)}):
        {fc['filtered_text']}

        ## ìš”ì•½ ì§€ì¹¨:
        1. ìœ„ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ {dynamic_length}ì ë‚´ì™¸ë¡œ ìš”ì•½í•˜ì„¸ìš”.
        2. ì¤‘ìš”í•˜ì§€ ì•Šì€ ì„¸ë¶€ì‚¬í•­ì€ ê³¼ê°íˆ ìƒëµí•˜ì„¸ìš”.
        3. ì›ë¬¸ì˜ í•µì‹¬ ê°œë…ê³¼ ì£¼ìš” ì•„ì´ë””ì–´ë¥¼ ë³´ì¡´í•˜ì„¸ìš”.
        4. ìš”ì•½ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì—°ê²°ì„±ì„ ìœ ì§€í•˜ì„¸ìš”.
        5. ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
        6. ëª¨ë“  ë¬¸ì¥ì€ ì™„ê²°ë˜ê²Œ ì‘ì„±í•˜ê³ , ì¤‘ê°„ì— ëŠê¸°ì§€ ì•Šë„ë¡ í•˜ì„¸ìš”.
        7. ìš”ì•½ì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.

        ## ìš”ì•½ ê²°ê³¼:"""
                        chunk_prompts.append(prompt)
            except Exception as e:
                logger.error(f"ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                semantic_extraction_used = False

        # ì„ë² ë”© ê²€ìƒ‰ ì‹¤íŒ¨í•˜ê±°ë‚˜ ì‚¬ìš©í•  ìˆ˜ ì—†ëŠ” ê²½ìš° ìƒì„±í˜• ìš”ì•½ ë°©ì‹ ì‚¬ìš©
        if not semantic_extraction_used:
            for i, chunk in enumerate(chunks):
                # í•´ë‹¹ ì²­í¬ì— ëŒ€í•œ ë™ì  í• ë‹¹ í¬ê¸° ì‚¬ìš©
                dynamic_length = token_allocations.get(i, chars_per_chunk)

                # ë™ì  í• ë‹¹ í¬ê¸°ë¥¼ í”„ë¡¬í”„íŠ¸ì— ë°˜ì˜
                prompt = f"""# í…ìŠ¤íŠ¸ ìš”ì•½ ì‘ì—…

        ## ì›ë³¸ í…ìŠ¤íŠ¸ (íŒŒíŠ¸ {i+1}/{len(chunks)}):
        {chunk.text}

        ## ìš”ì•½ ì§€ì¹¨:
        1. ìœ„ í…ìŠ¤íŠ¸ì˜ í•µì‹¬ ë‚´ìš©ë§Œ ì¶”ì¶œí•˜ì—¬ {dynamic_length}ì ë‚´ì™¸ë¡œ ìš”ì•½í•˜ì„¸ìš”.
        2. ì¤‘ìš”í•˜ì§€ ì•Šì€ ì„¸ë¶€ì‚¬í•­ì€ ê³¼ê°íˆ ìƒëµí•˜ì„¸ìš”.
        3. ì›ë¬¸ì˜ í•µì‹¬ ê°œë…ê³¼ ì£¼ìš” ì•„ì´ë””ì–´ë¥¼ ë³´ì¡´í•˜ì„¸ìš”.
        4. ìš”ì•½ì€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê³ , ë¬¸ì¥ ì‚¬ì´ì— ì ì ˆí•œ ì—°ê²°ì„±ì„ ìœ ì§€í•˜ì„¸ìš”.
        5. ì›ë¬¸ì— ì—†ëŠ” ë‚´ìš©ì„ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”.
        6. ëª¨ë“  ë¬¸ì¥ì€ ì™„ê²°ë˜ê²Œ ì‘ì„±í•˜ê³ , ì¤‘ê°„ì— ëŠê¸°ì§€ ì•Šë„ë¡ í•˜ì„¸ìš”.
        7. ìš”ì•½ì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œë¡œ ëë‚˜ì•¼ í•©ë‹ˆë‹¤.

        ## ìš”ì•½ ê²°ê³¼:"""
                chunk_prompts.append(prompt)

        # ë³‘ë ¬ ìš”ì•½ ìƒì„± - ë¬¸ì ìˆ˜ë¥¼ í† í° ìˆ˜ë¡œ ë³€í™˜ (í•œêµ­ì–´ì˜ ê²½ìš° ë” ë†’ì€ ë¹„ìœ¨)
        # ë™ì  í† í° í• ë‹¹ ì •ë³´ ê³„ì‚° (í”„ë¡¬í”„íŠ¸ ì¸ë±ìŠ¤ â†’ í† í° ìˆ˜ ë§¤í•‘)
        dynamic_tokens = {}

        prompt_index = 0
        if semantic_extraction_used:
            # ì„ë² ë”© ì‚¬ìš© ì‹œ: filtered_chunksì˜ ìˆœì„œëŒ€ë¡œ í”„ë¡¬í”„íŠ¸ê°€ ìƒì„±ë¨
            for fc in filtered_chunks:
                chunk_idx = fc["index"]
                chars = token_allocations.get(chunk_idx, chars_per_chunk)
                # ë¬¸ì ìˆ˜ë¥¼ í† í° ìˆ˜ë¡œ ë³€í™˜ (í•œêµ­ì–´ ê³ ë ¤)
                tokens = max(120, int(chars * 1.0))
                dynamic_tokens[prompt_index] = tokens
                prompt_index += 1
        else:
            # ì„ë² ë”© ë¯¸ì‚¬ìš© ì‹œ: chunks ìˆœì„œëŒ€ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±ë¨
            for i in range(len(chunks)):
                chars = token_allocations.get(i, chars_per_chunk)
                tokens = max(120, int(chars * 1.0))
                dynamic_tokens[prompt_index] = tokens
                prompt_index += 1

        # ëª¨ë“  í”„ë¡¬í”„íŠ¸ì— ëŒ€í•œ í† í° í• ë‹¹ ì •ë³´ ë¡œê¹…
        logger.info(f"ì²­í¬ë³„ ë™ì  í† í° í• ë‹¹ (ìµœì¢…): {dynamic_tokens}")
        print(f"ğŸ”„ ë™ì  í† í° í• ë‹¹ìœ¼ë¡œ ì²­í¬ ìš”ì•½ ìƒì„± ì¤‘...")

        # ê°œë³„ ì²­í¬ë§ˆë‹¤ ë‹¤ë¥¸ í† í° ìˆ˜ í• ë‹¹ì„ ìœ„í•œ í•¨ìˆ˜
        def get_tokens_for_prompt(idx):
            # ì •ìˆ˜ ê°’ ë°˜í™˜ ë³´ì¥ (JSON ì§ë ¬í™” ê°€ëŠ¥)
            return int(max(40, dynamic_tokens.get(idx, 120)))  # ìµœì†Œ 40í† í° ë³´ì¥

        # ì²­í¬ë³„ë¡œ ë‹¤ë¥¸ í† í° ìˆ˜ë¥¼ ì ìš©í•œ ë³‘ë ¬ ìƒì„±
        chunk_summaries = await self.triton.generate_parallel_optimized(
            chunk_prompts, max_new_tokens=get_tokens_for_prompt
        )

        # ìµœì¢… í†µí•© ìš”ì•½
        combined_text = "\n\n".join(
            [s for s in chunk_summaries if s and not s.startswith("[ì˜¤ë¥˜")]
        )  # ì˜¤ë¥˜ ì‘ë‹µ í•„í„°ë§

        # ìš”ì•½ ë¸”ë¡ ë° ë¬¸ì¥ ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        if combined_text.strip():
            blocks = [b.strip() for b in combined_text.split("\n\n") if b.strip()]
            unique_blocks = list(dict.fromkeys(blocks))
            cleaned_blocks = []
            for block in unique_blocks:
                lines = [ln.strip() for ln in block.split("\n") if ln.strip()]
                dedup_lines = list(dict.fromkeys(lines))
                cleaned_blocks.append("\n".join(dedup_lines))
            combined_text = "\n\n".join(cleaned_blocks)

        # ë¬¸ì¥ ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        if combined_text.strip():
            lines = combined_text.split("\n")
            deduped_lines = list(dict.fromkeys(lines))  # ìˆœì„œë¥¼ ìœ ì§€í•˜ë©° ì¤‘ë³µ ì œê±°
            deduped_lines = [line for line in deduped_lines if line.strip()]
            combined_text = "\n".join(deduped_lines)

        if not combined_text.strip():
            logger.warning("ëª¨ë“  ì²­í¬ ìš”ì•½ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ì§ì ‘ ìš”ì•½ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            # ëª¨ë“  ì²­í¬ ìš”ì•½ì´ ì‹¤íŒ¨í•œ ê²½ìš° ì›ë³¸ í…ìŠ¤íŠ¸ì—ì„œ ê°„ë‹¨í•œ ìš”ì•½ ì¶”ì¶œ
            # ì„ë² ë”© ê¸°ë°˜ í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì‹œë„
            if self.semantic_engine is not None:
                try:
                    # í•µì‹¬ ë¬¸ì¥ ì¶”ì¶œ ì¿¼ë¦¬
                    extraction_query = "ì´ ë¬¸ì„œì˜ í•µì‹¬ ë‚´ìš©ê³¼ ì¤‘ìš” ì •ë³´ë¥¼ ìš”ì•½"
                    # ìƒìœ„ 20ê°œ ê´€ë ¨ ë¬¸ì¥ ì¶”ì¶œ (ì¤‘ë³µ ì œê±° ì ìš©)
                    filtered_text = self.semantic_engine.extract_relevant_context(
                        extraction_query, text, top_k=20, dedup_threshold=0.85
                    )
                    if filtered_text.strip():
                        short_text = filtered_text
                        logger.info(
                            f"ì„ë² ë”© í•„í„°ë§ìœ¼ë¡œ ìµœì¢… ìš”ì•½ìš© í…ìŠ¤íŠ¸ ì¶”ì¶œ ì„±ê³µ: {len(filtered_text)} ë¬¸ì"
                        )
                    else:
                        short_text = text[
                            : min(len(text), 2000)
                        ]  # ì›ë³¸ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
                except Exception as e:
                    logger.error(f"ì„ë² ë”© ê¸°ë°˜ ìµœì¢… í•„í„°ë§ ì¤‘ ì˜¤ë¥˜: {e}")
                    short_text = text[
                        : min(len(text), 2000)
                    ]  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ í…ìŠ¤íŠ¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©
            else:
                short_text = text[
                    : min(len(text), 2000)
                ]  # ì˜ë¯¸ ê²€ìƒ‰ ì—”ì§„ ì—†ì„ ê²½ìš° ì›ë³¸ ì•ë¶€ë¶„ë§Œ ì‚¬ìš©

            final_prompt = f"ë‹¤ìŒ í…ìŠ¤íŠ¸ë¥¼ {target_length}ì ì´ë‚´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”:\n\n{short_text}"
        else:
            # ëª©í‘œ ìš”ì•½ ê¸¸ì´ ì¦ê°€ (ë” ìƒì„¸í•œ ìš”ì•½ì„ ìœ„í•´)
            enhanced_target_length = int(target_length * 1.5)  # 50% ëŠ˜ë¦° ìš”ì•½ ê¸¸ì´

            final_prompt = f"""# ë¬¸ì„œ ìš”ì•½ ìƒì„±

        ## ìš”ì•½ ì‘ì—…:
        ë‹¤ìŒì€ ê¸´ ë¬¸ì„œì˜ íŒŒíŠ¸ë³„ ìš”ì•½ì…ë‹ˆë‹¤. ì´ ìš”ì•½ë“¤ì„ í†µí•©í•˜ì—¬ ì „ì²´ ë‚´ìš©ì„ ë‹´ì€ ìµœì¢… ìš”ì•½ì„ ì‘ì„±í•´ì£¼ì„¸ìš”.

        ## íŒŒíŠ¸ë³„ ìš”ì•½:
        {combined_text}

        ## ìš”ì•½ ì¡°ê±´:
        - ìš”ì•½ ê¸¸ì´: {enhanced_target_length}ì ë‚´ì™¸ë¡œ ì‘ì„±í•  ê²ƒ
        - ë¬¸ì„œì˜ ì£¼ì œ, í•µì‹¬ ë…¼ì , ì£¼ìš” ë‚´ìš©ì„ í¬í•¨í•  ê²ƒ
        - ë…¼ë¦¬ì  íë¦„ì„ ìœ ì§€í•˜ê³  ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•  ê²ƒ
        - ê° íŒŒíŠ¸ì˜ í•µì‹¬ë§Œ ì¶”ì¶œí•˜ì—¬ í†µí•©í•  ê²ƒ
        - ëª¨ë“  ë¬¸ì¥ì€ ì™„ê²°ë˜ê²Œ ì‘ì„±í•˜ê³ , ë¬¸ì¥ì´ ì¤‘ê°„ì— ì˜ë¦¬ì§€ ì•Šë„ë¡ í•  ê²ƒ
        - ìš”ì•½ì˜ ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ë°˜ë“œì‹œ ë§ˆì¹¨í‘œë¡œ ëë‚˜ë„ë¡ í•  ê²ƒ

        ## ìµœì¢… ìš”ì•½:"""

            # ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ë¡œê¹…
            try:
                from pathlib import Path
                from config.settings import LOG_DIR

                # ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸
                prompt_log_dir = LOG_DIR / "prompts"
                os.makedirs(prompt_log_dir, exist_ok=True)

                # íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
                timestamp = time.strftime("%Y%m%d_%H%M%S")
                final_log_file = prompt_log_dir / f"final_prompt_{timestamp}.log"

                # ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ì €ì¥
                with open(final_log_file, "w", encoding="utf-8") as f:
                    f.write("==== ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ====\n\n")
                    f.write(final_prompt)

                logger.info(f"ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ë¡œê·¸ ì €ì¥ë¨: {final_log_file}")
                print(f"â„¹ï¸ ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ë¡œê·¸ ì €ì¥ë¨: {final_log_file}")
            except Exception as e:
                logger.warning(f"ìµœì¢… ìš”ì•½ í”„ë¡¬í”„íŠ¸ ë¡œê¹… ì‹¤íŒ¨: {e}")

            # ìµœì¢… ìš”ì•½ì´ ê³¼ë„í•˜ê²Œ ê¸¸ì–´ì§€ì§€ ì•Šë„ë¡ í† í° ìˆ˜ ì¡°ì •
            approx_tokens = max(150, int(enhanced_target_length * 2.5))

            print(f"\nğŸ”„ ìµœì¢… ìš”ì•½ ìƒì„± ì¤‘... (ìµœëŒ€ {approx_tokens} í† í° í• ë‹¹)")
            final_start_time = time.time()
            # í† í° ì •ë³´ ë¡œê¹… í™œì„±í™”
            final_summary = await self.triton._generate_single_cached(
                final_prompt, approx_tokens, log_tokens=True, log_prompt=True
            )
            final_time = time.time() - final_start_time
            print(f"âœ… ìµœì¢… ìš”ì•½ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {final_time:.2f}ì´ˆ)")

        # ì²˜ë¦¬ ë°©ë²• ê²°ì •
        processing_method = "hierarchical"
        if semantic_extraction_used:
            processing_method = "hierarchical_with_embedding_filter"

        # ì„ë² ë”© ì •ë³´ ìˆ˜ì§‘ (ì‚¬ìš©ëœ ê²½ìš°)
        embedding_info = None
        if semantic_extraction_used and hasattr(self.semantic_engine, "embedding_info"):
            embedding_info = self.semantic_engine.embedding_info
            logger.info(f"ì„ë² ë”© ì •ë³´ ìˆ˜ì§‘ë¨: {embedding_info}")

        return {
            "chunk_summaries": chunk_summaries,
            "final_summary": final_summary,
            "processing_method": processing_method,
            "semantic_extraction_used": semantic_extraction_used,
            "embedding_info": embedding_info,
            "dynamic_token_allocation": token_allocations,
            "original_chunk_count": original_chunk_count,
            "selected_chunk_indices": selected_indices,
        }

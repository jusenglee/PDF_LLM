import asyncio
import hashlib
import json
import logging
import aiohttp
from aiohttp import ClientError, ClientResponseError, ClientTimeout
from src.utils.cache import ResponseCache

logger = logging.getLogger(__name__)

class OptimizedTritonClient:
    MAX_SERVER_TOKENS = 1024

    def __init__(self, url: str, batch: int = 16, timeout: float = 60.0, max_connections: int = 32, tokenizer=None):
        self.url = url.rstrip("/")
        self.sem = asyncio.Semaphore(batch)
        self.cache = ResponseCache()
        self.timeout = timeout
        self.batch = batch
        self.max_connections = max_connections
        self._session = None
        self.tokenizer = tokenizer

    def _trim_for_server(self, user_text: str) -> str:
        """ìµœì¢… í† í° ìˆ˜ê°€ MAX_SERVER_TOKENS ì´í•˜ê°€ ë˜ë„ë¡ user_textë¥¼ ê°€ìš´ë°ë¥¼ ì˜ë¼ ì¶•ì•½"""
        tok = self.tokenizer
        if not tok:
            return user_text

        # â‘  í…œí”Œë¦¿ 'ë¨¸ë¦¬Â·ê¼¬ë¦¬' ê¸¸ì´(ê³ ì •ë¶„) ë¯¸ë¦¬ ê³„ì‚°
        empty_tpl = tok.apply_chat_template(
            [{"role": "user", "content": ""}],
            tokenize=False, add_generation_prompt=True
        )
        head_tail_tokens = len(tok.encode(empty_tpl, add_special_tokens=False))

        # â‘¡ user_text ë¥¼ í† í°í™”
        ids_user = tok.encode(user_text, add_special_tokens=False)
        max_user_tokens = self.MAX_SERVER_TOKENS - head_tail_tokens - 2   # 2-í† í° ì—¬ìœ 

        if len(ids_user) <= max_user_tokens:
            return user_text                        # ì´ë¯¸ ì•ˆì „

        # â‘¢ ê°€ìš´ë° ì˜ë¼ë‚´ê¸°
        keep_front = max_user_tokens // 2
        keep_back  = max_user_tokens - keep_front
        new_ids = ids_user[:keep_front] + ids_user[-keep_back:]

        return tok.decode(new_ids, skip_special_tokens=False)

    @property
    def session(self) -> aiohttp.ClientSession:
        if self._session is None or self._session.closed:
            connector = aiohttp.TCPConnector(
                limit=self.max_connections,
                limit_per_host=self.max_connections,
                keepalive_timeout=30.0,
                enable_cleanup_closed=True
            )
            timeout = aiohttp.ClientTimeout(total=self.timeout, connect=10.0)
            self._session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                raise_for_status=True  # ìë™ìœ¼ë¡œ HTTP ì˜¤ë¥˜ ë°œìƒ
            )
        return self._session

    async def __aenter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì… - ì„¸ì…˜ ì´ˆê¸°í™” ë³´ì¥"""
        # ì„¸ì…˜ì´ ë‹«í˜”ê±°ë‚˜ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± (ì†ì„± ì ‘ê·¼ì í™œìš©)
        if getattr(self, '_session', None) is None or self._session.closed:
            _ = self.session  # ì„¸ì…˜ ì´ˆê¸°í™” íŠ¸ë¦¬ê±°
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ - ì•ˆì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.close()

    async def close(self):
        """ì•ˆì „í•œ ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            if self._session and not self._session.closed:
                await self._session.close()
                # ì„¸ì…˜ì´ ì™„ì „íˆ ì •ë¦¬ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
                await asyncio.sleep(0.1)
        except Exception as e:
            logger.debug(f"ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
        finally:
            self._session = None

        try:
            await self.cache.close()
        except Exception as e:
            logger.debug(f"ìºì‹œ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

    async def generate_parallel_optimized(
        self,
        prompts: list[str],
        *,
        max_new_tokens: int = 160,
        dynamic_batch_size: bool = True
    ) -> list[str]:
        """ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •ê³¼ íŒŒì´í”„ë¼ì´ë‹ì„ í†µí•œ ìµœì í™”ëœ ë³‘ë ¬ ì²˜ë¦¬"""
        if not prompts:
            return []

        # ë™ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        if dynamic_batch_size:
            avg_prompt_length = sum(len(p) for p in prompts) / len(prompts)
            if avg_prompt_length < 500:
                effective_batch = min(self.batch * 2, 32)  # ì§§ì€ í”„ë¡¬í”„íŠ¸ëŠ” ë” ë§ì´
            else:
                effective_batch = max(self.batch // 2, 4)   # ê¸´ í”„ë¡¬í”„íŠ¸ëŠ” ì ê²Œ
        else:
            effective_batch = self.batch

        # íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì²­í¬ ë¶„í• 
        chunks = [prompts[i:i + effective_batch] for i in range(0, len(prompts), effective_batch)]
        results = [None] * len(prompts)

        async def process_chunk(chunk_prompts: list[str], start_idx: int, log_first_prompt: bool = False):
            tasks = []
            for i, prompt in enumerate(chunk_prompts):
                # ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ëŠ” í† í° ì •ë³´ ë¡œê¹… í™œì„±í™” (ì„ íƒì )
                log_this_prompt = log_first_prompt and i == 0 and start_idx == 0

                # ê° í”„ë¡¬í”„íŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” íƒœìŠ¤í¬ ìƒì„±
                task = asyncio.create_task(
                    self._generate_single_cached(prompt, max_new_tokens, log_tokens=log_this_prompt, log_prompt=log_this_prompt),
                    name=f"prompt_{start_idx + i}"
                )
                tasks.append(task)

            # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
            chunk_results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ë¥¼ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ë°°ì¹˜
            for i, result in enumerate(chunk_results):
                if isinstance(result, Exception):
                    logger.error(f"í”„ë¡¬í”„íŠ¸ {start_idx + i} ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                    results[start_idx + i] = "[Error]"
                else:
                    results[start_idx + i] = result

        # ì²­í¬ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì œí•œ)
        sem = asyncio.Semaphore(3)  # ìµœëŒ€ 3ê°œ ì²­í¬ ë™ì‹œ ì²˜ë¦¬

        async def process_chunk_with_limit(chunk, idx):
            chunk_num = idx + 1
            total_chunks = len(chunks)
            print(f"  ğŸ“Š ì²­í¬ ê·¸ë£¹ {chunk_num}/{total_chunks} ì²˜ë¦¬ ì¤‘... ({len(chunk)}/{len(prompts)} í”„ë¡¬í”„íŠ¸)")
            async with sem:
                # ì²« ë²ˆì§¸ ì²­í¬ì˜ ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ë§Œ ë¡œê¹… (idx == 0)
                await process_chunk(chunk, idx * effective_batch, log_first_prompt=(idx == 0))
            print(f"  âœ“ ì²­í¬ ê·¸ë£¹ {chunk_num}/{total_chunks} ì™„ë£Œ")

        # ëª¨ë“  ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ì„¸ë§ˆí¬ì–´ë¡œ ì œí•œ)
        await asyncio.gather(*[
            process_chunk_with_limit(chunk, i) 
            for i, chunk in enumerate(chunks)
        ])

        return [r for r in results if r is not None]

    def _log_prompt_to_file(self, prompt: str, prefix: str = "prompt"):
        """í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ íŒŒì¼ì— ë¡œê¹…"""
        try:
            import os
            import time
            from pathlib import Path
            from config.settings import LOG_DIR

            # ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸
            prompt_log_dir = LOG_DIR / "prompts"
            os.makedirs(prompt_log_dir, exist_ok=True)

            # íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            log_file = prompt_log_dir / f"{prefix}_{timestamp}.log"

            # í”„ë¡¬í”„íŠ¸ ì €ì¥
            with open(log_file, "w", encoding="utf-8") as f:
                f.write(prompt)

            logger.debug(f"í”„ë¡¬í”„íŠ¸ ë¡œê·¸ ì €ì¥ë¨: {log_file}")
            return str(log_file)
        except Exception as e:
            logger.warning(f"í”„ë¡¬í”„íŠ¸ ë¡œê¹… ì‹¤íŒ¨: {e}")
            return None

    async def _generate_single_cached(self, prompt: str, max_new_tokens: int, log_tokens: bool = False, log_prompt: bool = False) -> str:
        """ìºì‹œë¥¼ í™œìš©í•œ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ - aiohttp ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ ë°©ì‹"""
        # TensorRT-LLM ìµœëŒ€ ì…ë ¥ ì œí•œ ì²˜ë¦¬ (2047 í† í°)
        MAX_INPUT_TOKENS = 2047
        TOTAL_MODEL_CONTEXT = 4096  # ëª¨ë¸ì˜ ì´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´

        # í† í¬ë‚˜ì´ì €ê°€ ìˆì„ ê²½ìš°ì—ë§Œ ê¸¸ì´ ì œí•œ ì ìš©
        if self.tokenizer:
            try:
                # ì›ë³¸ í† í° ìˆ˜ í™•ì¸
                original_token_count = len(self.tokenizer(prompt, add_special_tokens=False).input_ids)

                # í”„ë¡¬í”„íŠ¸ ì¶•ì†Œ ì „ì— ë¡œê¹… ì˜µì…˜ í™œì„±í™”ëœ ê²½ìš° ì›ë³¸ í† í° ì •ë³´ ì¶œë ¥
                if log_tokens and original_token_count > MAX_INPUT_TOKENS:
                    print(f"\nğŸ“Š í”„ë¡¬í”„íŠ¸ í† í° ì •ë³´ (ì¶•ì†Œ ì „):")
                    print(f"  â€¢ ì›ë³¸ ì…ë ¥ í† í° ìˆ˜: {original_token_count}")
                    print(f"  â€¢ ìµœëŒ€ í—ˆìš© í† í°: {MAX_INPUT_TOKENS}")
                    print(f"  âš ï¸ í† í° ì´ˆê³¼: {original_token_count - MAX_INPUT_TOKENS}í† í° ì´ˆê³¼ë¡œ ìë™ ì¶•ì†Œë¨")

                # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ ì ìš©
                prompt = self._trim_for_server(prompt)

                # ì¶•ì†Œ í›„ í† í° ìˆ˜ ë‹¤ì‹œ ê³„ì‚°
                token_count = len(self.tokenizer(prompt, add_special_tokens=False).input_ids)

                # í† í° ìˆ˜ ë¡œê¹… ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš° í† í° ì •ë³´ ì¶œë ¥
                if log_tokens:
                    available_tokens = MAX_INPUT_TOKENS - token_count
                    response_tokens = min(max_new_tokens, TOTAL_MODEL_CONTEXT - MAX_INPUT_TOKENS)

                    print(f"\nğŸ“Š í”„ë¡¬í”„íŠ¸ í† í° ì •ë³´:")
                    print(f"  â€¢ ì…ë ¥ í† í° ìˆ˜: {token_count}")
                    print(f"  â€¢ ìµœëŒ€ í—ˆìš© í† í°: {MAX_INPUT_TOKENS}")
                    print(f"  â€¢ ì‘ë‹µ ê°€ëŠ¥ í† í°: {available_tokens}")
                    print(f"  â€¢ ìš”ì²­ ì‘ë‹µ í† í°: {max_new_tokens}")
                    print(f"  â€¢ ì‹¤ì œ ì‚¬ìš© í† í°: {response_tokens}")

                    # í† í° ì´ˆê³¼ ê°€ëŠ¥ì„± ê²½ê³  ë° ìë™ ì¡°ì •
                    if max_new_tokens > available_tokens:
                        print(f"  âš ï¸ ì£¼ì˜: ìš”ì²­ëœ ì‘ë‹µ í† í°({max_new_tokens})ì´ ì‚¬ìš© ê°€ëŠ¥í•œ í† í°({available_tokens})ë³´ë‹¤ ë§ìŠµë‹ˆë‹¤.")
                        print(f"  âš ï¸ ëª¨ë¸ì€ ìµœëŒ€ {response_tokens}ê°œ í† í°ë§Œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                    # í”„ë¡¬í”„íŠ¸ ë¡œê¹…
                    log_file = self._log_prompt_to_file(prompt, f"prompt_{token_count}tokens")
                    if log_file:
                        print(f"  ğŸ“ í”„ë¡¬í”„íŠ¸ ë¡œê·¸: {log_file}")

                if token_count > MAX_INPUT_TOKENS:
                    logger.warning(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì´ˆê³¼: {token_count} í† í° > {MAX_INPUT_TOKENS} ì œí•œ (ìë™ ì¶•ì†Œ ì‹¤íŒ¨)")

                    # ì§€ì‹œë¬¸ê³¼ ë‚´ìš© ë¶„ë¦¬ (ì§€ì‹œë¬¸ì€ ë³´ì¡´)
                    parts = prompt.split("\n\n", 1)
                    instruction = parts[0] if len(parts) > 1 else ""
                    content = parts[1] if len(parts) > 1 else parts[0]

                    # ì§€ì‹œë¬¸ í† í° ìˆ˜ ê³„ì‚°
                    instruction_tokens = len(self.tokenizer(instruction, add_special_tokens=False).input_ids)

                    # ë‚´ìš©ì— í• ë‹¹í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ìˆ˜ ê³„ì‚°
                    available_tokens = MAX_INPUT_TOKENS - instruction_tokens - 5  # ì—¬ìœ  í† í°

                    # ë‚´ìš©ì„ í† í° ë‹¨ìœ„ë¡œ ìë¥´ê¸°
                    encoded_content = self.tokenizer(content, add_special_tokens=False)
                    truncated_content_ids = encoded_content.input_ids[:available_tokens]
                    truncated_content = self.tokenizer.decode(truncated_content_ids)

                    # ì§€ì‹œë¬¸ê³¼ ì˜ë¦° ë‚´ìš© ì¬ê²°í•©
                    if instruction:
                        prompt = f"{instruction}\n\n{truncated_content}"
                    else:
                        prompt = truncated_content

                    # ìµœì¢… í† í° ìˆ˜ ë¡œê¹…
                    final_tokens = len(self.tokenizer(prompt, add_special_tokens=False).input_ids)
                    logger.info(f"ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {final_tokens} í† í° (ì¶•ì†Œ í›„)")
            except Exception as e:
                logger.warning(f"í† í° ê¸¸ì´ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

        key = hashlib.sha256(f"{prompt}:{max_new_tokens}".encode()).hexdigest()

        # ìºì‹œ í™•ì¸
        try:
            cached = await self.cache.get(key)
            if cached is not None:
                return cached
        except Exception as e:
            logger.debug(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
        async with self.sem:
            full_text = []

            # 3ë²ˆê¹Œì§€ ì¬ì‹œë„
            for attempt in range(3):
                try:
                    headers = {"Content-Type": "application/json", "Accept": "text/event-stream"}
                    data = {"text_input": prompt, "max_tokens": max_new_tokens}

                    async with self.session.post(
                        self.url,
                        json=data,
                        headers=headers,
                        timeout=self.timeout
                    ) as response:
                        response.raise_for_status()

                        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ (chunked)
                        async for chunk in response.content.iter_chunked(1024):
                            chunk_text = chunk.decode("utf-8").strip()

                            # ì¤„ ë‹¨ìœ„ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬
                            for line in chunk_text.split('\n'):
                                if not line or line.isspace():
                                    continue

                                # SSE í˜•ì‹ ì²˜ë¦¬
                                if line.startswith("event:"):
                                    continue
                                if line.startswith("data: "):
                                    try:
                                        json_str = line[6:].strip()
                                        data = json.loads(json_str)
                                        text = data.get("text_output", "")
                                        if text:
                                            full_text.append(text)
                                    except json.JSONDecodeError as e:
                                        # ë” ìì„¸í•œ ì˜¤ë¥˜ ì •ë³´ ë¡œê¹…
                                        logger.debug(f"ì‘ë‹µ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {line!r}")
                                        logger.debug(f"JSONDecodeError: {e}, ìœ„ì¹˜: {e.pos}, ë¼ì¸: {e.lineno}, ì—´: {e.colno}")
                                        continue
                                    except KeyError as e:
                                        logger.debug(f"ì‘ë‹µ ë°ì´í„°ì—ì„œ í•„ìš”í•œ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {e}")
                                        continue
                                    except Exception as e:
                                        logger.debug(f"ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                                        continue

                    # ì‘ë‹µ ê²°í•©
                    result = "".join(full_text)
                    if result:
                        await self.cache.put(key, result)
                        return result
                    else:
                        # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
                        raise ValueError("Triton ì„œë²„ê°€ ë¹ˆ ì‘ë‹µì„ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤")

                except Exception as e:
                    error_msg = str(e)
                    if "Prompt length" in error_msg and "exceeds maximum input length" in error_msg:
                        logger.error(f"ì…ë ¥ ê¸¸ì´ ì´ˆê³¼ ì˜¤ë¥˜ (ì„œë²„ ì¸¡): {e}")
                        return "[ì˜¤ë¥˜: ì…ë ¥ ê¸¸ì´ê°€ ì„œë²„ ì œí•œì„ ì´ˆê³¼í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ê°€ ìë™ìœ¼ë¡œ ì˜ë ¸ì§€ë§Œ ì„œë²„ ì¸¡ì—ì„œ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤.]"

                    if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                        logger.error(f"Triton ì„œë²„ í˜¸ì¶œ ì‹¤íŒ¨ (ìµœì¢…): {e}")
                        return f"[ì¶”ë¡  ì˜¤ë¥˜: {str(e)[:50]}]" if len(str(e)) > 50 else f"[ì¶”ë¡  ì˜¤ë¥˜: {e}]"

                    # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                    logger.warning(f"Triton ì„œë²„ í˜¸ì¶œ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt+1}/3): {e}")
                    await asyncio.sleep(0.5 * (attempt + 1))

            # ì¬ì‹œë„ ì‹¤íŒ¨ ì²˜ë¦¬
            return "[Triton ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜]"

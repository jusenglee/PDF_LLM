import asyncio
import hashlib
import json
import logging
import aiohttp
from aiohttp import ClientError, ClientResponseError, ClientTimeout
from src.utils.cache import ResponseCache

logger = logging.getLogger(__name__)

class OptimizedTritonClient:
    MAX_SERVER_TOKENS = 2048

    def __init__(self, url: str, batch: int = 16, timeout: float = 60.0, max_connections: int = 32, tokenizer=None):
        self.url = url.rstrip("/")
        self.sem = asyncio.Semaphore(batch)
        self.cache = ResponseCache()
        self.timeout = timeout
        self.batch = batch
        self.max_connections = max_connections
        self._session = None
        self.tokenizer = tokenizer

    def _trim_for_server(self, user_text: str) -> str:
        """ìµœì¢… í† í° ìˆ˜ê°€ MAX_SERVER_TOKENS ì´í•˜ê°€ ë˜ë„ë¡ user_textë¥¼ ê°€ìš´ë°ë¥¼ ì˜ë¼ ì¶•ì•½"""
        tok = self.tokenizer
        if not tok:
            return user_text

        # â‘  í…œí”Œë¦¿ 'ë¨¸ë¦¬Â·ê¼¬ë¦¬' ê¸¸ì´(ê³ ì •ë¶„) ë¯¸ë¦¬ ê³„ì‚°
        empty_tpl = tok.apply_chat_template(
            [{"role": "user", "content": ""}],
            tokenize=False, add_generation_prompt=True
        )
        head_tail_tokens = len(tok.encode(empty_tpl, add_special_tokens=False))

        # â‘¡ user_text ë¥¼ í† í°í™”
        ids_user = tok.encode(user_text, add_special_tokens=False)
        max_user_tokens = self.MAX_SERVER_TOKENS - head_tail_tokens - 2   # 2-í† í° ì—¬ìœ 

        if len(ids_user) <= max_user_tokens:
            return user_text                        # ì´ë¯¸ ì•ˆì „

        # â‘¢ ê°€ìš´ë° ì˜ë¼ë‚´ê¸°
        keep_front = max_user_tokens // 2
        keep_back  = max_user_tokens - keep_front
        new_ids = ids_user[:keep_front] + ids_user[-keep_back:]

        return tok.decode(new_ids, skip_special_tokens=False)

    @property
    def session(self) -> aiohttp.ClientSession:
        if self._session is None or self._session.closed:
            connector = aiohttp.TCPConnector(
                limit=self.max_connections,
                limit_per_host=self.max_connections,
                keepalive_timeout=30.0,
                enable_cleanup_closed=True
            )
            timeout = aiohttp.ClientTimeout(total=self.timeout, connect=10.0)
            self._session = aiohttp.ClientSession(
                connector=connector,
                timeout=timeout,
                raise_for_status=True  # ìë™ìœ¼ë¡œ HTTP ì˜¤ë¥˜ ë°œìƒ
            )
        return self._session

    async def __aenter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì… - ì„¸ì…˜ ì´ˆê¸°í™” ë³´ì¥"""
        # ì„¸ì…˜ì´ ë‹«í˜”ê±°ë‚˜ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„± (ì†ì„± ì ‘ê·¼ì í™œìš©)
        if getattr(self, '_session', None) is None or self._session.closed:
            _ = self.session  # ì„¸ì…˜ ì´ˆê¸°í™” íŠ¸ë¦¬ê±°
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ - ì•ˆì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        await self.close()

    async def close(self):
        """ì•ˆì „í•œ ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            if self._session and not self._session.closed:
                await self._session.close()
                # ì„¸ì…˜ì´ ì™„ì „íˆ ì •ë¦¬ë  ë•Œê¹Œì§€ ì ì‹œ ëŒ€ê¸°
                await asyncio.sleep(0.1)
        except Exception as e:
            logger.debug(f"ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
        finally:
            self._session = None

        try:
            await self.cache.close()
        except Exception as e:
            logger.debug(f"ìºì‹œ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

    async def generate_parallel_optimized(
        self,
        prompts: list[str],
        *,
        max_new_tokens: int = 160,
        dynamic_batch_size: bool = True
    ) -> list[str]:
        """ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •ê³¼ íŒŒì´í”„ë¼ì´ë‹ì„ í†µí•œ ìµœì í™”ëœ ë³‘ë ¬ ì²˜ë¦¬"""
        if not prompts:
            return []

        # ë™ì  ë°°ì¹˜ í¬ê¸° ê³„ì‚°
        if dynamic_batch_size:
            avg_prompt_length = sum(len(p) for p in prompts) / len(prompts)
            if avg_prompt_length < 500:
                effective_batch = min(self.batch * 2, 32)  # ì§§ì€ í”„ë¡¬í”„íŠ¸ëŠ” ë” ë§ì´
            else:
                effective_batch = max(self.batch // 2, 4)   # ê¸´ í”„ë¡¬í”„íŠ¸ëŠ” ì ê²Œ
        else:
            effective_batch = self.batch

        # íŒŒì´í”„ë¼ì¸ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì²­í¬ ë¶„í• 
        chunks = [prompts[i:i + effective_batch] for i in range(0, len(prompts), effective_batch)]
        results = [None] * len(prompts)

        async def process_chunk(chunk_prompts: list[str], start_idx: int, log_first_prompt: bool = False):
            tasks = []
            for i, prompt in enumerate(chunk_prompts):
                # ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ëŠ” í† í° ì •ë³´ ë¡œê¹… í™œì„±í™” (ì„ íƒì )
                log_this_prompt = log_first_prompt and i == 0 and start_idx == 0

                # ê° í”„ë¡¬í”„íŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ëŠ” íƒœìŠ¤í¬ ìƒì„±
                task = asyncio.create_task(
                    self._generate_single_cached(prompt, max_new_tokens, log_tokens=log_this_prompt, log_prompt=log_this_prompt),
                    name=f"prompt_{start_idx + i}"
                )
                tasks.append(task)

            # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸°
            chunk_results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ë¥¼ ì˜¬ë°”ë¥¸ ìœ„ì¹˜ì— ë°°ì¹˜
            for i, result in enumerate(chunk_results):
                if isinstance(result, Exception):
                    logger.error(f"í”„ë¡¬í”„íŠ¸ {start_idx + i} ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
                    results[start_idx + i] = "[Error]"
                else:
                    results[start_idx + i] = result

        # ì²­í¬ë“¤ì„ ìˆœì°¨ì ìœ¼ë¡œ ì²˜ë¦¬ (ìµœëŒ€ ë™ì‹œ ì²˜ë¦¬ ìˆ˜ ì œí•œ)
        sem = asyncio.Semaphore(3)  # ìµœëŒ€ 3ê°œ ì²­í¬ ë™ì‹œ ì²˜ë¦¬

        async def process_chunk_with_limit(chunk, idx):
            chunk_num = idx + 1
            total_chunks = len(chunks)
            print(f"  ğŸ“Š ì²­í¬ ê·¸ë£¹ {chunk_num}/{total_chunks} ì²˜ë¦¬ ì¤‘... ({len(chunk)}/{len(prompts)} í”„ë¡¬í”„íŠ¸)")
            async with sem:
                # ì²« ë²ˆì§¸ ì²­í¬ì˜ ì²« ë²ˆì§¸ í”„ë¡¬í”„íŠ¸ë§Œ ë¡œê¹… (idx == 0)
                await process_chunk(chunk, idx * effective_batch, log_first_prompt=(idx == 0))
            print(f"  âœ“ ì²­í¬ ê·¸ë£¹ {chunk_num}/{total_chunks} ì™„ë£Œ")

        # ëª¨ë“  ì²­í¬ ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘ (ì„¸ë§ˆí¬ì–´ë¡œ ì œí•œ)
        await asyncio.gather(*[
            process_chunk_with_limit(chunk, i) 
            for i, chunk in enumerate(chunks)
        ])

        return [r for r in results if r is not None]

    def _get_cache_key(self, prompt: str, max_new_tokens: int, temperature: float, top_p: float) -> str:
        """í”„ë¡¬í”„íŠ¸ì™€ íŒŒë¼ë¯¸í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìºì‹œ í‚¤ ìƒì„±"""
        # ì½œë°± í•¨ìˆ˜ê°€ ì „ë‹¬ëœ ê²½ìš° ê¸°ë³¸ê°’ìœ¼ë¡œ ëŒ€ì²´
        if callable(max_new_tokens):
            max_new_tokens = 200  # ì•ˆì „í•œ ê¸°ë³¸ê°’

        prompt_hash = hashlib.sha256(prompt.encode('utf-8')).hexdigest()[:16]
        params_str = f"{max_new_tokens}_{temperature:.2f}_{top_p:.2f}"
        return f"{prompt_hash}_{params_str}"

    def _log_prompt_to_file(self, prompt: str, prefix: str = "prompt"):
        """í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì„ íŒŒì¼ì— ë¡œê¹…"""
        try:
            import os
            import time
            from pathlib import Path
            from config.settings import LOG_DIR

            # ë¡œê·¸ ë””ë ‰í† ë¦¬ í™•ì¸
            prompt_log_dir = LOG_DIR / "prompts"
            os.makedirs(prompt_log_dir, exist_ok=True)

            # íƒ€ì„ìŠ¤íƒ¬í”„ë¡œ ë¡œê·¸ íŒŒì¼ëª… ìƒì„±
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            log_file = prompt_log_dir / f"{prefix}_{timestamp}.log"

            # í”„ë¡¬í”„íŠ¸ ì €ì¥
            with open(log_file, "w", encoding="utf-8") as f:
                f.write(prompt)

            logger.debug(f"í”„ë¡¬í”„íŠ¸ ë¡œê·¸ ì €ì¥ë¨: {log_file}")
            return str(log_file)
        except Exception as e:
            logger.warning(f"í”„ë¡¬í”„íŠ¸ ë¡œê¹… ì‹¤íŒ¨: {e}")
            return None

    async def _generate_single_cached(self, prompt: str, max_new_tokens: int, log_tokens: bool = False, log_prompt: bool = False) -> str:
        """ìºì‹œë¥¼ í™œìš©í•œ ë‹¨ì¼ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬ - aiohttp ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ ë°©ì‹"""
        # TensorRT-LLM ìµœëŒ€ ì…ë ¥ ì œí•œ ì²˜ë¦¬ (2047 í† í°)
        MAX_INPUT_TOKENS = 2048
        TOTAL_MODEL_CONTEXT = 4096  # ëª¨ë¸ì˜ ì´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´

        # ì•ˆì „ ë§ˆì§„ (í† í° ê³„ì‚°ì˜ ì˜¤ì°¨ë¥¼ ê³ ë ¤) - ì—¬ìœ  ìˆê²Œ ì„¤ì •
        SAFETY_MARGIN = 100
        # ì¶œë ¥ í† í° ìµœëŒ€ í•œê³„ (í•œê³„ ì´ˆê³¼ ë°©ì§€)
        MAX_OUTPUT_TOKENS = 900

        # í† í¬ë‚˜ì´ì €ê°€ ìˆì„ ê²½ìš°ì—ë§Œ ê¸¸ì´ ì œí•œ ì ìš©
        if self.tokenizer:
            try:
                # ì›ë³¸ í† í° ìˆ˜ í™•ì¸
                original_token_count = len(self.tokenizer(prompt, add_special_tokens=False).input_ids)

                # í”„ë¡¬í”„íŠ¸ ì¶•ì†Œ ì „ì— ë¡œê¹… ì˜µì…˜ í™œì„±í™”ëœ ê²½ìš° ì›ë³¸ í† í° ì •ë³´ ì¶œë ¥
                if log_tokens and original_token_count > MAX_INPUT_TOKENS:
                    print(f"\nğŸ“Š í”„ë¡¬í”„íŠ¸ í† í° ì •ë³´ (ì¶•ì†Œ ì „):")
                    print(f"  â€¢ ì›ë³¸ ì…ë ¥ í† í° ìˆ˜: {original_token_count}")
                    print(f"  â€¢ ìµœëŒ€ í—ˆìš© í† í°: {MAX_INPUT_TOKENS}")
                    print(f"  âš ï¸ í† í° ì´ˆê³¼: {original_token_count - MAX_INPUT_TOKENS}í† í° ì´ˆê³¼ë¡œ ìë™ ì¶•ì†Œë¨")

                # í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì œí•œ ì ìš©
                prompt = self._trim_for_server(prompt)

                # ì¶•ì†Œ í›„ í† í° ìˆ˜ ë‹¤ì‹œ ê³„ì‚°
                token_count = len(self.tokenizer(prompt, add_special_tokens=False).input_ids)

                # í† í° ìˆ˜ ë¡œê¹… ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš° í† í° ì •ë³´ ì¶œë ¥
                # ì´ í† í°ì´ ëª¨ë¸ í•œê³„ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ìë™ ì¡°ì •
                total_possible_tokens = TOTAL_MODEL_CONTEXT - SAFETY_MARGIN
                available_tokens = total_possible_tokens - token_count

                # ìš”ì²­ í† í°ì´ ê°€ìš© í† í°ì„ ì´ˆê³¼í•˜ë©´ ìë™ ì¡°ì •
                # callable ì²˜ë¦¬ ì¶”ê°€
                tokens_to_check = max_new_tokens
                if callable(max_new_tokens):
                    try:
                        # ì„ì˜ì˜ ì¸ë±ìŠ¤ë¡œ í‰ê°€ (ì‹¤ì œ ê°’ì´ í•„ìš”í•œ ê²½ìš°)
                        tokens_to_check = max_new_tokens(0)
                    except Exception as e:
                        logger.warning(f"í† í° ê¸¸ì´ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
                        tokens_to_check = 200  # ì•ˆì „í•œ ê¸°ë³¸ê°’

                if token_count + tokens_to_check > total_possible_tokens:
                    adjusted_tokens = max(100, min(available_tokens, MAX_OUTPUT_TOKENS))  # ìµœì†Œ 100í† í°, ìµœëŒ€ MAX_OUTPUT_TOKENS ë³´ì¥
                    logger.info(f"í† í° ìë™ ì¡°ì •: {tokens_to_check} â†’ {adjusted_tokens} (ì…ë ¥: {token_count}, ê°€ìš©: {available_tokens})")
                    max_new_tokens = adjusted_tokens
                # ìš”ì²­ì´ ìµœëŒ€ ì¶œë ¥ í•œê³„ë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš°ì—ë„ ì œí•œ ì ìš©
                elif not callable(max_new_tokens) and max_new_tokens > MAX_OUTPUT_TOKENS:
                    logger.info(f"ìµœëŒ€ í† í° ì œí•œ ì ìš©: {max_new_tokens} â†’ {MAX_OUTPUT_TOKENS}")
                    max_new_tokens = MAX_OUTPUT_TOKENS

                if log_tokens:
                    print(f"\nğŸ“Š í”„ë¡¬í”„íŠ¸ í† í° ì •ë³´:")
                    print(f"  â€¢ ì…ë ¥ í† í° ìˆ˜: {token_count}")
                    print(f"  â€¢ ìµœëŒ€ í—ˆìš© ì»¨í…ìŠ¤íŠ¸: {TOTAL_MODEL_CONTEXT}")
                    print(f"  â€¢ ì•ˆì „ ì»¨í…ìŠ¤íŠ¸ í•œê³„: {total_possible_tokens}")
                    print(f"  â€¢ ì‘ë‹µ ê°€ëŠ¥ í† í°: {available_tokens}")
                    print(f"  â€¢ ìš”ì²­ ì‘ë‹µ í† í°: {max_new_tokens}")

                    # í† í° ìë™ ì¡°ì • ì •ë³´ í‘œì‹œ
                    if token_count + max_new_tokens > TOTAL_MODEL_CONTEXT:
                        print(f"  âš ï¸ ì£¼ì˜: ì´ í† í°({token_count + max_new_tokens})ì´ ëª¨ë¸ í•œê³„({TOTAL_MODEL_CONTEXT})ë¥¼ ì´ˆê³¼í•©ë‹ˆë‹¤.")
                        print(f"  ğŸ”„ ì‘ë‹µ í† í°ì´ {max_new_tokens}ê°œë¡œ ìë™ ì¡°ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")

                    # í”„ë¡¬í”„íŠ¸ ë¡œê¹…
                    log_file = self._log_prompt_to_file(prompt, f"prompt_{token_count}tokens")
                    if log_file:
                        print(f"  ğŸ“ í”„ë¡¬í”„íŠ¸ ë¡œê·¸: {log_file}")

                if token_count > MAX_INPUT_TOKENS:
                    logger.warning(f"í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ì´ˆê³¼: {token_count} í† í° > {MAX_INPUT_TOKENS} ì œí•œ (ìë™ ì¶•ì†Œ ì‹¤íŒ¨)")

                    # ì§€ì‹œë¬¸ê³¼ ë‚´ìš© ë¶„ë¦¬ (ì§€ì‹œë¬¸ì€ ë³´ì¡´)
                    parts = prompt.split("\n\n", 1)
                    instruction = parts[0] if len(parts) > 1 else ""
                    content = parts[1] if len(parts) > 1 else parts[0]

                    # ì§€ì‹œë¬¸ í† í° ìˆ˜ ê³„ì‚°
                    instruction_tokens = len(self.tokenizer(instruction, add_special_tokens=False).input_ids)

                    # ë‚´ìš©ì— í• ë‹¹í•  ìˆ˜ ìˆëŠ” ìµœëŒ€ í† í° ìˆ˜ ê³„ì‚°
                    available_tokens = MAX_INPUT_TOKENS - instruction_tokens - 5  # ì—¬ìœ  í† í°

                    # ë‚´ìš©ì„ í† í° ë‹¨ìœ„ë¡œ ìë¥´ê¸°
                    encoded_content = self.tokenizer(content, add_special_tokens=False)
                    truncated_content_ids = encoded_content.input_ids[:available_tokens]
                    truncated_content = self.tokenizer.decode(truncated_content_ids)

                    # ì§€ì‹œë¬¸ê³¼ ì˜ë¦° ë‚´ìš© ì¬ê²°í•©
                    if instruction:
                        prompt = f"{instruction}\n\n{truncated_content}"
                    else:
                        prompt = truncated_content

                    # ìµœì¢… í† í° ìˆ˜ ë¡œê¹…
                    final_tokens = len(self.tokenizer(prompt, add_special_tokens=False).input_ids)
                    logger.info(f"ìµœì¢… í”„ë¡¬í”„íŠ¸ ê¸¸ì´: {final_tokens} í† í° (ì¶•ì†Œ í›„)")
            except Exception as e:
                logger.warning(f"í† í° ê¸¸ì´ ê³„ì‚° ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

        key = hashlib.sha256(f"{prompt}:{max_new_tokens}".encode()).hexdigest()

        # ìºì‹œ í™•ì¸
        try:
            cached = await self.cache.get(key)
            if cached is not None:
                return cached
        except Exception as e:
            logger.debug(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨: {e}")

        # ì‹¤ì œ ìŠ¤íŠ¸ë¦¬ë° API í˜¸ì¶œ
        async with self.sem:
            full_text = []

            # 3ë²ˆê¹Œì§€ ì¬ì‹œë„
            for attempt in range(3):
                try:
                    headers = {"Content-Type": "application/json", "Accept": "text/event-stream"}
                    data = {"text_input": prompt, "max_tokens": max_new_tokens}

                    # API í˜¸ì¶œ ì „ì— ëª¨ë¸ ìµœëŒ€ ì»¨í…ìŠ¤íŠ¸ ì´ˆê³¼ ì—¬ë¶€ ìµœì¢… í™•ì¸
                    if self.tokenizer:
                        try:
                            input_tokens = len(self.tokenizer(prompt, add_special_tokens=False).input_ids)
                            if input_tokens + max_new_tokens > TOTAL_MODEL_CONTEXT - SAFETY_MARGIN:
                                # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì¶œë ¥ í† í° ì¡°ì •
                                safe_tokens = max(50, min(TOTAL_MODEL_CONTEXT - SAFETY_MARGIN - input_tokens, MAX_OUTPUT_TOKENS))
                                logger.warning(f"API í˜¸ì¶œ ì „ í† í° ì•ˆì „ ì¡°ì •: {max_new_tokens} â†’ {safe_tokens} (ì…ë ¥: {input_tokens})")
                                data["max_tokens"] = safe_tokens  # API ìš”ì²­ ë°ì´í„° ìˆ˜ì •
                        except Exception as e:
                            logger.warning(f"í† í° ì•ˆì „ ì¡°ì • ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")

                    async with self.session.post(
                        self.url,
                        json=data,
                        headers=headers,
                        timeout=self.timeout
                    ) as response:
                        response.raise_for_status()

                        # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ ê°œì„  - ì²­í¬ í¬ê¸° ì¦ê°€ ë° ë²„í¼ ë³´ê°•
                        buffer = ""
                        last_incomplete = ""
                        try:
                            # ì²­í¬ í¬ê¸°ë¥¼ ëŠ˜ë ¤ ìŠ¤íŠ¸ë¦¬ë° ì•ˆì •ì„± í–¥ìƒ
                            async for chunk in response.content.iter_chunked(4096):
                                try:
                                    chunk_text = chunk.decode("utf-8")
                                    # ì´ì „ì— ì²˜ë¦¬í•˜ì§€ ëª»í•œ ë¶ˆì™„ì „ ë¼ì¸ê³¼ í˜„ì¬ ì²­í¬ ê²°í•©
                                    buffer = last_incomplete + chunk_text
                                    lines = buffer.split('\n')
                                    # ë§ˆì§€ë§‰ ë¼ì¸ì´ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë”°ë¡œ ì €ì¥
                                    last_incomplete = lines.pop() if lines else ""

                                    # ì™„ì „í•œ ë¼ì¸ë“¤ ì²˜ë¦¬
                                    for line in lines:
                                        line = line.strip()
                                        if not line or line.isspace():
                                            continue

                                        # SSE í˜•ì‹ ì²˜ë¦¬
                                        if line.startswith("event:"):
                                            continue
                                        if line.startswith("data: "):
                                            try:
                                                json_str = line[6:].strip()
                                                data = json.loads(json_str)
                                                text = data.get("text_output", "")
                                                if text:
                                                    full_text.append(text)
                                            except json.JSONDecodeError as e:
                                                # ë¶ˆì™„ì „í•œ JSONì¼ ê°€ëŠ¥ì„± - ë¡œê¹…ë§Œ í•˜ê³  ê³„ì† ì§„í–‰
                                                logger.debug(f"ì‘ë‹µ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {line!r}")
                                                logger.debug(f"JSONDecodeError: {e}, ìœ„ì¹˜: {e.pos}")
                                                continue
                                            except KeyError as e:
                                                logger.debug(f"ì‘ë‹µ ë°ì´í„°ì—ì„œ í•„ìš”í•œ í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {e}")
                                                continue
                                            except Exception as e:
                                                logger.debug(f"ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {type(e).__name__}: {e}")
                                                continue
                                except UnicodeDecodeError as ude:
                                    logger.warning(f"ìœ ë‹ˆì½”ë“œ ë””ì½”ë”© ì˜¤ë¥˜ (ë¬´ì‹œ): {ude}")
                                    continue

                            # ìŠ¤íŠ¸ë¦¼ ì¢…ë£Œ í›„ ë§ˆì§€ë§‰ ë¶ˆì™„ì „ ë¼ì¸ ì²˜ë¦¬
                            if last_incomplete.strip():
                                if last_incomplete.startswith("data: "):
                                    try:
                                        json_str = last_incomplete[6:].strip()
                                        data = json.loads(json_str)
                                        text = data.get("text_output", "")
                                        if text:
                                            full_text.append(text)
                                    except Exception:
                                        pass  # ë§ˆì§€ë§‰ ë¶ˆì™„ì „ ë°ì´í„°ëŠ” ì¡°ìš©íˆ ë¬´ì‹œ
                        except Exception as e:
                            logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                            # ì—ëŸ¬ê°€ ë‚˜ë„ ì§€ê¸ˆê¹Œì§€ ë°›ì€ ë°ì´í„°ëŠ” ì²˜ë¦¬ ê³„ì†

                    # ì‘ë‹µ ê²°í•© - ë¬¸ìì—´ ì²˜ë¦¬ ê°œì„ 
                    if full_text:
                        # ë¬¸ìì—´ ê²°í•© ì‹œ strip/rstrip ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (í…ìŠ¤íŠ¸ ì˜ë¦¼ ë°©ì§€)
                        result = "".join(full_text)
                        # ì‘ë‹µì´ ìˆìœ¼ë©´ ìºì‹œ ë° ë°˜í™˜
                        if result:
                            await self.cache.put(key, result)
                            return result
                        else:
                            # ë¹ˆ ì‘ë‹µ ì²˜ë¦¬
                            raise ValueError("Triton ì„œë²„ê°€ ì‘ë‹µì„ ìƒì„±í–ˆìœ¼ë‚˜ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤")
                    else:
                        # ì‘ë‹µ ìì²´ê°€ ì—†ëŠ” ê²½ìš°
                        raise ValueError("Triton ì„œë²„ê°€ ì‘ë‹µì„ ë°˜í™˜í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")

                except Exception as e:
                    error_msg = str(e)
                    if "Prompt length" in error_msg and "exceeds maximum input length" in error_msg:
                        logger.error(f"ì…ë ¥ ê¸¸ì´ ì´ˆê³¼ ì˜¤ë¥˜ (ì„œë²„ ì¸¡): {e}")
                        return "[ì˜¤ë¥˜: ì…ë ¥ ê¸¸ì´ê°€ ì„œë²„ ì œí•œì„ ì´ˆê³¼í•©ë‹ˆë‹¤. í…ìŠ¤íŠ¸ê°€ ìë™ìœ¼ë¡œ ì˜ë ¸ì§€ë§Œ ì„œë²„ ì¸¡ì—ì„œ ê±°ë¶€í–ˆìŠµë‹ˆë‹¤.]"

                    if attempt == 2:  # ë§ˆì§€ë§‰ ì‹œë„
                        logger.error(f"Triton ì„œë²„ í˜¸ì¶œ ì‹¤íŒ¨ (ìµœì¢…): {e}")
                        return f"[ì¶”ë¡  ì˜¤ë¥˜: {str(e)[:50]}]" if len(str(e)) > 50 else f"[ì¶”ë¡  ì˜¤ë¥˜: {e}]"

                    # ì¬ì‹œë„ ì „ ëŒ€ê¸°
                    logger.warning(f"Triton ì„œë²„ í˜¸ì¶œ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt+1}/3): {e}")
                    await asyncio.sleep(0.5 * (attempt + 1))

            # ì¬ì‹œë„ ì‹¤íŒ¨ ì²˜ë¦¬
            return "[Triton ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜]"

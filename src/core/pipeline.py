import logging
import json
import time
from pathlib import Path
from transformers import AutoTokenizer
from src.utils.token_manager import AdaptiveTokenManager
from src.core.pdf_processor import AsyncPDFProcessor
from src.core.triton_client import OptimizedTritonClient
from src.core.summarizer import HierarchicalSummarizer
from config.settings import ROOT_DIR

logger = logging.getLogger(__name__)

class OptimizedPipeline:
    def __init__(
        self,
        model_path: str = "./",
        triton_url: str = "http://203.250.238.30:8888/v2/models/ensemble/generate_stream",
        ctx_len: int | None = None,
        config_path: str | Path | None = None,
    ):
        if ctx_len is None:
            # config.jsonì—ì„œ max_seq_len ê°’ì„ ì½ì–´ ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¡œ ì‚¬ìš©
            path = Path(config_path) if config_path else ROOT_DIR / "config.json"
            try:
                with open(path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                ctx_len = int(data.get("build_config", {}).get("max_seq_len", 2048))
            except Exception as e:
                logger.warning(f"config.json ë¡œë“œ ì‹¤íŒ¨: {e}")
                ctx_len = 2048

        self.token_mgr = AdaptiveTokenManager(model_path, ctx_len)
        self.pdf_proc = AsyncPDFProcessor()

        # í† í¬ë‚˜ì´ì € ê°€ì ¸ì˜¤ê¸°
        tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)
        if not tokenizer.pad_token:
            tokenizer.pad_token = tokenizer.eos_token
        # í† í¬ë‚˜ì´ì € ì£¼ì…
        self.triton = OptimizedTritonClient(triton_url, batch=16, tokenizer=tokenizer)
        self.summarizer = HierarchicalSummarizer(self.token_mgr, self.triton)


    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì§„ì…ì """
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì ì¢…ë£Œ - ìì› ì •ë¦¬"""
        await self.close()

    async def process_document_optimized(
        self,
        pdf_path: str,
        target_summary_length: int = 200
    ) -> dict:
        """ìµœì í™”ëœ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""

        # ì‹œì‘ ì‹œê°„ ê¸°ë¡
        start_time = time.time()
        process_times = {}

        try:
            # 1. PDF ì¶”ì¶œ
            extraction_start = time.time()
            try:
                raw_text = await self.pdf_proc.extract(pdf_path)
                process_times['extraction'] = time.time() - extraction_start
                if not raw_text.strip():
                    logger.warning(f"PDFì—ì„œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {pdf_path}")
                    print(f"âš ï¸ PDFì—ì„œ í…ìŠ¤íŠ¸ê°€ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤: {pdf_path}")
                    return {
                        "warning": "PDFì— ì¶”ì¶œ ê°€ëŠ¥í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤",
                        "success": False
                    }
            except Exception as e:
                logger.error(f"PDF ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                print(f"âŒ PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                return {
                    "error": f"PDF ì²˜ë¦¬ ì˜¤ë¥˜: {e}",
                    "success": False
                }

            # 2. í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ 
            clean_text = raw_text.strip()
            if not clean_text:
                raise ValueError("ì²˜ë¦¬í•  í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")

            # 3. ì ì‘ì  ì²­í‚¹
            chunking_start = time.time()
            try:
                chunks, allocation = self.token_mgr.create_adaptive_chunks(
                    clean_text, target_summary_length
                )
                process_times['chunking'] = time.time() - chunking_start
            except Exception as e:
                logger.error(f"í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹¤íŒ¨: {e}")
                print(f"âŒ ì²­í‚¹ ì˜¤ë¥˜: {e}")
                return {
                    "error": f"í…ìŠ¤íŠ¸ ì²­í‚¹ ì˜¤ë¥˜: {e}",
                    "success": False
                }

            # 4. ìš”ì•½ ì²˜ë¦¬
            summarizing_start = time.time()
            try:
                result = await self.summarizer.smart_chunking_summary(
                    clean_text, target_summary_length
                )
                process_times['summarizing'] = time.time() - summarizing_start
            except Exception as e:
                logger.error(f"ìš”ì•½ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                print(f"âŒ ìš”ì•½ ì˜¤ë¥˜: {e}")
                return {
                    "error": f"ìš”ì•½ ì²˜ë¦¬ ì˜¤ë¥˜: {e}",
                    "success": False
                }

            # 5. ì„±ê³µ ê²°ê³¼ ë°˜í™˜
            # ì„ë² ë”© ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            semantic_search_used = hasattr(self.summarizer, 'semantic_engine') and self.summarizer.semantic_engine is not None

            # ì„ë² ë”© ìƒíƒœ ëª…í™•íˆ ë¡œê¹…
            if semantic_search_used:
                logger.info("âœ… ì„ë² ë”© ëª¨ë“ˆ í™œì„±í™” ìƒíƒœë¡œ ì²˜ë¦¬ ì™„ë£Œ")
                print("âœ¨ ì„ë² ë”© ëª¨ë“ˆ í™œì„±í™”: ë¬¸ì„œ í•µì‹¬ ë‚´ìš© ì¶”ì¶œ ê¸°ëŠ¥ ì‚¬ìš© ì¤‘")
            else:
                logger.warning("âš ï¸ ì„ë² ë”© ëª¨ë“ˆ ë¹„í™œì„±í™” ìƒíƒœë¡œ ì²˜ë¦¬ ì™„ë£Œ (sentence-transformers, faiss-cpu ì„¤ì¹˜ í•„ìš”)")
                print("âš ï¸ ì„ë² ë”© ëª¨ë“ˆ ë¹„í™œì„±í™”: ì „ì²´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ëª¨ë“œë¡œ ë™ì‘ ì¤‘")
                print("ğŸ’¡ ì„ë² ë”© í™œì„±í™” ë°©ë²•: pip install sentence-transformers faiss-cpu")

            # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ê³„ì‚°
            total_time = time.time() - start_time

            result.update({
                "token_allocation": allocation,
                "success": True,
                "processing_stats": {
                    "chunks_created": len(chunks),
                    "avg_chunk_size": sum(c.token_count for c in chunks) / len(chunks),
                    "compression_ratio": len(result.get("final_summary", "")) / len(clean_text),
                    "semantic_search_used": semantic_search_used,
                    "process_times": process_times,
                    "total_time": total_time
                }
            })

            return result

        except Exception as e:
            logger.error(f"ë¬¸ì„œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return {
                "error": str(e),
                "success": False
            }

    async def close(self):
        """ì•ˆì „í•œ ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            await self.triton.close()
        except Exception as e:
            logger.debug(f"Triton í´ë¼ì´ì–¸íŠ¸ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")

        try:
            await self.pdf_proc.close()
        except Exception as e:
            logger.debug(f"PDF í”„ë¡œì„¸ì„œ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜ (ë¬´ì‹œë¨): {e}")
